{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IfsjshmjrdIY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torch import autograd\n",
    "import torch.nn.functional as F\n",
    "import os, os.path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchinfo import summary\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "import itertools\n",
    "import functools\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select type of data to train on\n",
    "dataset = 1 # Aggregate = 1, Planar = 2, Graupel = 3, Columnar = 4\n",
    "\n",
    "## Select correlation to use: w/ or w/out fractal dimension\n",
    "correlation = 2 # Holzer and Sommerfeld = 1, Mola = 2\n",
    "# Holzer and Sommerfeld fit to solid particles: use for planar, graupel, and columnar\n",
    "# Mola fit to aggregates: use for aggregates\n",
    "#\n",
    "# Mola may perform better sometimes on the other shapes, but results for fractal dimension are\n",
    "# non-physical on them: shape parameter outputs are less physically reliable\n",
    "\n",
    "## Physics-guided loss on or off\n",
    "custom_loss = 1 # 1 = True, 2 = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set directory for images and data\n",
    "dir_agg = '/Users/Crazz/Research Codes/CRREL files/Synthetic images/Aggregate_proj/aggregate_projections.csv'\n",
    "dir_plan = '/Users/Crazz/Research Codes/CRREL files/Synthetic images/Planar_proj/planar_projections.csv'\n",
    "dir_gr = '/Users/Crazz/Research Codes/CRREL files/Synthetic images/Graupel_proj/graupel_projections.csv'\n",
    "dir_col = '/Users/Crazz/Research Codes/CRREL files/Synthetic images/Columnar_proj/columnar_projections.csv'\n",
    "dir_imagg = '/Users/Crazz/Research Codes/CRREL files/Synthetic images/Aggregate_proj'\n",
    "dir_implan = '/Users/Crazz/Research Codes/CRREL files/Synthetic images/Planar_proj'\n",
    "dir_imgr = '/Users/Crazz/Research Codes/CRREL files/Synthetic images/Graupel_proj'\n",
    "dir_imcol = '/Users/Crazz/Research Codes/CRREL files/Synthetic images/Columnar_proj'\n",
    "## Set directory and name for saving model. Will overwrite a file of the same name\n",
    "dir_model = '/Users/Crazz/Research Codes/CRREL files/planhs5model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "def padding(img, expected_size):\n",
    "    desired_size = expected_size\n",
    "    delta_width = desired_size[0] - img.size[0]\n",
    "    delta_height = desired_size[1] - img.size[1]\n",
    "    pad_width = delta_width // 2\n",
    "    pad_height = delta_height // 2\n",
    "    padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n",
    "    return ImageOps.expand(img, padding)\n",
    "\n",
    "# Load synthetic planar data & images\n",
    "plan_geom = pd.read_csv(dir_plan)\n",
    "plan_geom = plan_geom.rename(columns={'img_name': 'id'})\n",
    "imgs = {}\n",
    "path = dir_implan\n",
    "valid_images = [\".jpg\",\".gif\",\".png\",\".tga\"]\n",
    "for f in os.listdir(path):\n",
    "    ext = os.path.splitext(f)[1]\n",
    "    if ext.lower() not in valid_images:\n",
    "        continue\n",
    "    temp = Image.open(os.path.join(path,f))\n",
    "    imgs[f] = temp.copy()\n",
    "    temp.close()\n",
    "plan_img = pd.DataFrame(imgs.items(), columns=['id', 'images'])\n",
    "plan_geom = plan_geom.merge(plan_img, how='inner', on='id')\n",
    "plan_geom.loc[plan_geom.porosity==0,['porosity']] = 0.01\n",
    "\n",
    "# Load synthetic graupel data & images\n",
    "gr_geom = pd.read_csv(dir_gr)\n",
    "gr_geom['img_name'] = gr_geom['img_name'] + '.png'\n",
    "gr_geom = gr_geom.rename(columns={'img_name': 'id'})\n",
    "imgs = {}\n",
    "path = dir_imgr\n",
    "valid_images = [\".jpg\",\".gif\",\".png\",\".tga\"]\n",
    "for f in os.listdir(path):\n",
    "    ext = os.path.splitext(f)[1]\n",
    "    if ext.lower() not in valid_images:\n",
    "        continue\n",
    "    temp = Image.open(os.path.join(path,f))\n",
    "    imgs[f] = temp.copy()\n",
    "    temp.close()\n",
    "gr_img = pd.DataFrame(imgs.items(), columns=['id', 'images'])\n",
    "gr_geom = gr_geom.merge(gr_img, how='inner', on='id')\n",
    "\n",
    "# Load synthetic aggregate data & images\n",
    "agg_geom = pd.read_csv(dir_agg)\n",
    "agg_geom = agg_geom.drop(['Unnamed: 0'], axis=1)\n",
    "agg_geom = agg_geom.rename(columns={'image filename': 'id'})\n",
    "agg_geom = agg_geom.drop(['convex hull minor axis length','convex hull perimeter','rotation around x-axis','rotation around y-axis','image resolution','area','perimeter','convex hull major axis length','rotation around z-axis','Feret diameter orthogonal to maximum Feret diameter','number of monomers','liquid water path'],axis=1)\n",
    "imgs = {}\n",
    "path = dir_imagg\n",
    "valid_images = [\".jpg\",\".gif\",\".png\",\".tga\"]\n",
    "for f in os.listdir(path):\n",
    "    ext = os.path.splitext(f)[1]\n",
    "    if ext.lower() not in valid_images:\n",
    "        continue\n",
    "    temp = Image.open(os.path.join(path,f))\n",
    "    imgs[f] = temp.copy()\n",
    "    temp.close()\n",
    "img_df = pd.DataFrame(imgs.items(), columns=['id', 'images'])\n",
    "agg_geom = agg_geom.merge(img_df, how='inner', on='id')\n",
    "\n",
    "# Load synthetic columnar data and images\n",
    "col_geom = pd.read_csv(dir_col)\n",
    "col_geom = col_geom.rename(columns={'img_name': 'id'})\n",
    "col_geom['id'] = col_geom['id'] + '.png'\n",
    "imgs = {}\n",
    "path = dir_imcol\n",
    "valid_images = [\".jpg\",\".gif\",\".png\",\".tga\"]\n",
    "for f in os.listdir(path):\n",
    "    ext = os.path.splitext(f)[1]\n",
    "    if ext.lower() not in valid_images:\n",
    "        continue\n",
    "    temp = Image.open(os.path.join(path,f))\n",
    "    imgs[f] = temp.copy()\n",
    "    temp.close()\n",
    "col_img = pd.DataFrame(imgs.items(), columns=['id', 'images'])\n",
    "col_geom = col_geom.merge(col_img, how='inner', on='id')\n",
    "col_geom.loc[col_geom.porosity==0,['porosity']] = 0.01\n",
    "\n",
    "# create index to be fed to the NN\n",
    "plan_geom['tmp_idx'] = np.arange(0,len(plan_geom),1)\n",
    "gr_geom['tmp_idx'] = np.arange(0,len(gr_geom),1)\n",
    "agg_geom['tmp_idx'] = np.arange(0,len(agg_geom),1)\n",
    "col_geom['tmp_idx'] = np.arange(0,len(col_geom),1)\n",
    "#normalize shape param data- here only a linear normalization from 0-1: divide by the max expected value\n",
    "plan_geom['por_n'] = (plan_geom['porosity'] - 0)/(1-0)\n",
    "plan_geom['sar_n'] = (plan_geom['sar'] - 0)/(0.5-0)\n",
    "plan_geom['sph_n'] = (plan_geom['sph'] - 0)/(1-0)\n",
    "plan_geom['csph_n'] = (plan_geom['cross_sph'] - 0)/(3.5-0)\n",
    "plan_geom['lsph_n'] = (plan_geom['length_sph'] - 0)/(2-0)\n",
    "plan_geom['Acrat_n'] = (plan_geom['A_crat'] - 0)/(1.5-0)\n",
    "plan_geom['Alrat_n'] = (plan_geom['A_lrat'] - 0)/(1.5-0)\n",
    "plan_geom['Df_n'] = plan_geom['Df']/3\n",
    "gr_geom['por_n'] = (gr_geom['porosity'] - 0)/(1-0)\n",
    "gr_geom['sar_n'] = (gr_geom['sar'] - 0)/(0.5-0)\n",
    "gr_geom['sph_n'] = (gr_geom['sph'] - 0)/(1-0)\n",
    "gr_geom['csph_n'] = (gr_geom['cross_sph'] - 0)/(3.5-0)\n",
    "gr_geom['lsph_n'] = (gr_geom['length_sph'] - 0)/(2-0)\n",
    "gr_geom['Acrat_n'] = (gr_geom['A_crat'] - 0)/(1.5-0)\n",
    "gr_geom['Alrat_n'] = (gr_geom['A_lrat'] - 0)/(1.5-0)\n",
    "gr_geom['Df_n'] = gr_geom['Df']/3\n",
    "agg_geom['por_n'] = (agg_geom['porosity'] - 0)/(1-0)\n",
    "agg_geom['sar_n'] = (agg_geom['sar'] - 0)/(0.5-0)\n",
    "agg_geom['sph_n'] = (agg_geom['sph'] - 0)/(1-0)\n",
    "agg_geom['csph_n'] = (agg_geom['cross_sph'] - 0)/(3.5-0)\n",
    "agg_geom['lsph_n'] = (agg_geom['length_sph'] - 0)/(2-0)\n",
    "agg_geom['Acrat_n'] = (agg_geom['A_crat'] - 0)/(1.5-0)\n",
    "agg_geom['Alrat_n'] = (agg_geom['A_lrat'] - 0)/(1.5-0)\n",
    "agg_geom['Df_n'] = agg_geom['Df']/3\n",
    "col_geom['por_n'] = (col_geom['porosity'] - 0)/(1-0)\n",
    "col_geom['sar_n'] = (col_geom['sar'] - 0)/(0.5-0)\n",
    "col_geom['sph_n'] = (col_geom['sph'] - 0)/(1-0)\n",
    "col_geom['csph_n'] = (col_geom['cross_sph'] - 0)/(3.5-0)\n",
    "col_geom['lsph_n'] = (col_geom['length_sph'] - 0)/(2-0)\n",
    "col_geom['Acrat_n'] = (col_geom['A_crat'] - 0)/(1.5-0)\n",
    "col_geom['Alrat_n'] = (col_geom['A_lrat'] - 0)/(1.5-0)\n",
    "col_geom['Df_n'] = col_geom['Df']/3\n",
    "# Manually split aggregate data so that the problem is not trivial for the NN\n",
    "agg_geom['snowflake_class_id'] = 4\n",
    "agg_geom_train = agg_geom[(agg_geom['ID']<160.0)].reset_index(drop=True) # roughly 77% - 23% train test split\n",
    "agg_geom_test = agg_geom[(agg_geom['ID']>=160.0)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OUQWxRrqrdIk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 196\n",
      "Test set size: 49\n",
      "Epoch 1/600 - Train geom Loss: 1.255643 - Test Loss: 1.269276\n",
      "Epoch 1/600 - Train phys geom Loss: 0.649049 - Test Loss: 0.639289\n",
      "Epoch 1/600 - Train por loss: 0.1148 - Test loss: 0.1738\n",
      "Epoch 1/600 - Train sar loss: 0.0426 - Test loss: 0.0399\n",
      "Epoch 1/600 - Train sph loss: 0.2343 - Test loss: 0.2176\n",
      "Epoch 1/600 - Train c_sph loss: 0.3127 - Test loss: 0.2980\n",
      "Epoch 1/600 - Train l_sph loss: 0.3185 - Test loss: 0.3083\n",
      "Epoch 1/600 - Train Acrat loss: 0.1072 - Test loss: 0.1105\n",
      "Epoch 1/600 - Train Alrat loss: 0.1255 - Test loss: 0.1210\n",
      "\n",
      "\n",
      "Epoch 2/600 - Train geom Loss: 1.242366 - Test Loss: 1.256449\n",
      "Epoch 2/600 - Train phys geom Loss: 0.625209 - Test Loss: 0.616338\n",
      "Epoch 2/600 - Train por loss: 0.1148 - Test loss: 0.1731\n",
      "Epoch 2/600 - Train sar loss: 0.0423 - Test loss: 0.0394\n",
      "Epoch 2/600 - Train sph loss: 0.2317 - Test loss: 0.2152\n",
      "Epoch 2/600 - Train c_sph loss: 0.3078 - Test loss: 0.2933\n",
      "Epoch 2/600 - Train l_sph loss: 0.3166 - Test loss: 0.3068\n",
      "Epoch 2/600 - Train Acrat loss: 0.1045 - Test loss: 0.1080\n",
      "Epoch 2/600 - Train Alrat loss: 0.1247 - Test loss: 0.1207\n",
      "\n",
      "\n",
      "Epoch 3/600 - Train geom Loss: 1.227833 - Test Loss: 1.243884\n",
      "Epoch 3/600 - Train phys geom Loss: 0.599082 - Test Loss: 0.591903\n",
      "Epoch 3/600 - Train por loss: 0.1147 - Test loss: 0.1726\n",
      "Epoch 3/600 - Train sar loss: 0.0414 - Test loss: 0.0385\n",
      "Epoch 3/600 - Train sph loss: 0.2291 - Test loss: 0.2124\n",
      "Epoch 3/600 - Train c_sph loss: 0.3015 - Test loss: 0.2884\n",
      "Epoch 3/600 - Train l_sph loss: 0.3155 - Test loss: 0.3063\n",
      "Epoch 3/600 - Train Acrat loss: 0.1015 - Test loss: 0.1050\n",
      "Epoch 3/600 - Train Alrat loss: 0.1242 - Test loss: 0.1207\n",
      "\n",
      "\n",
      "Epoch 4/600 - Train geom Loss: 1.213284 - Test Loss: 1.227616\n",
      "Epoch 4/600 - Train phys geom Loss: 0.572345 - Test Loss: 0.565968\n",
      "Epoch 4/600 - Train por loss: 0.1140 - Test loss: 0.1712\n",
      "Epoch 4/600 - Train sar loss: 0.0403 - Test loss: 0.0373\n",
      "Epoch 4/600 - Train sph loss: 0.2252 - Test loss: 0.2083\n",
      "Epoch 4/600 - Train c_sph loss: 0.2961 - Test loss: 0.2824\n",
      "Epoch 4/600 - Train l_sph loss: 0.3154 - Test loss: 0.3056\n",
      "Epoch 4/600 - Train Acrat loss: 0.0979 - Test loss: 0.1023\n",
      "Epoch 4/600 - Train Alrat loss: 0.1243 - Test loss: 0.1204\n",
      "\n",
      "\n",
      "Epoch 5/600 - Train geom Loss: 1.198751 - Test Loss: 1.210600\n",
      "Epoch 5/600 - Train phys geom Loss: 0.544721 - Test Loss: 0.537935\n",
      "Epoch 5/600 - Train por loss: 0.1136 - Test loss: 0.1695\n",
      "Epoch 5/600 - Train sar loss: 0.0395 - Test loss: 0.0359\n",
      "Epoch 5/600 - Train sph loss: 0.2216 - Test loss: 0.2038\n",
      "Epoch 5/600 - Train c_sph loss: 0.2901 - Test loss: 0.2761\n",
      "Epoch 5/600 - Train l_sph loss: 0.3149 - Test loss: 0.3055\n",
      "Epoch 5/600 - Train Acrat loss: 0.0949 - Test loss: 0.0987\n",
      "Epoch 5/600 - Train Alrat loss: 0.1241 - Test loss: 0.1211\n",
      "\n",
      "\n",
      "Epoch 6/600 - Train geom Loss: 1.184264 - Test Loss: 1.194589\n",
      "Epoch 6/600 - Train phys geom Loss: 0.522569 - Test Loss: 0.512423\n",
      "Epoch 6/600 - Train por loss: 0.1135 - Test loss: 0.1687\n",
      "Epoch 6/600 - Train sar loss: 0.0380 - Test loss: 0.0345\n",
      "Epoch 6/600 - Train sph loss: 0.2165 - Test loss: 0.1990\n",
      "Epoch 6/600 - Train c_sph loss: 0.2841 - Test loss: 0.2706\n",
      "Epoch 6/600 - Train l_sph loss: 0.3151 - Test loss: 0.3053\n",
      "Epoch 6/600 - Train Acrat loss: 0.0922 - Test loss: 0.0953\n",
      "Epoch 6/600 - Train Alrat loss: 0.1249 - Test loss: 0.1213\n",
      "\n",
      "\n",
      "Epoch 7/600 - Train geom Loss: 1.168133 - Test Loss: 1.177939\n",
      "Epoch 7/600 - Train phys geom Loss: 0.500901 - Test Loss: 0.488793\n",
      "Epoch 7/600 - Train por loss: 0.1131 - Test loss: 0.1673\n",
      "Epoch 7/600 - Train sar loss: 0.0366 - Test loss: 0.0332\n",
      "Epoch 7/600 - Train sph loss: 0.2103 - Test loss: 0.1939\n",
      "Epoch 7/600 - Train c_sph loss: 0.2789 - Test loss: 0.2647\n",
      "Epoch 7/600 - Train l_sph loss: 0.3147 - Test loss: 0.3053\n",
      "Epoch 7/600 - Train Acrat loss: 0.0895 - Test loss: 0.0922\n",
      "Epoch 7/600 - Train Alrat loss: 0.1250 - Test loss: 0.1214\n",
      "\n",
      "\n",
      "Epoch 8/600 - Train geom Loss: 1.146663 - Test Loss: 1.160338\n",
      "Epoch 8/600 - Train phys geom Loss: 0.469384 - Test Loss: 0.465946\n",
      "Epoch 8/600 - Train por loss: 0.1121 - Test loss: 0.1648\n",
      "Epoch 8/600 - Train sar loss: 0.0347 - Test loss: 0.0318\n",
      "Epoch 8/600 - Train sph loss: 0.2047 - Test loss: 0.1886\n",
      "Epoch 8/600 - Train c_sph loss: 0.2706 - Test loss: 0.2588\n",
      "Epoch 8/600 - Train l_sph loss: 0.3137 - Test loss: 0.3051\n",
      "Epoch 8/600 - Train Acrat loss: 0.0854 - Test loss: 0.0897\n",
      "Epoch 8/600 - Train Alrat loss: 0.1255 - Test loss: 0.1217\n",
      "\n",
      "\n",
      "Epoch 9/600 - Train geom Loss: 1.130766 - Test Loss: 1.140479\n",
      "Epoch 9/600 - Train phys geom Loss: 0.446929 - Test Loss: 0.440243\n",
      "Epoch 9/600 - Train por loss: 0.1112 - Test loss: 0.1627\n",
      "Epoch 9/600 - Train sar loss: 0.0333 - Test loss: 0.0302\n",
      "Epoch 9/600 - Train sph loss: 0.1998 - Test loss: 0.1825\n",
      "Epoch 9/600 - Train c_sph loss: 0.2650 - Test loss: 0.2522\n",
      "Epoch 9/600 - Train l_sph loss: 0.3137 - Test loss: 0.3041\n",
      "Epoch 9/600 - Train Acrat loss: 0.0826 - Test loss: 0.0868\n",
      "Epoch 9/600 - Train Alrat loss: 0.1252 - Test loss: 0.1220\n",
      "\n",
      "\n",
      "Epoch 10/600 - Train geom Loss: 1.113971 - Test Loss: 1.120570\n",
      "Epoch 10/600 - Train phys geom Loss: 0.424754 - Test Loss: 0.416709\n",
      "Epoch 10/600 - Train por loss: 0.1106 - Test loss: 0.1609\n",
      "Epoch 10/600 - Train sar loss: 0.0318 - Test loss: 0.0284\n",
      "Epoch 10/600 - Train sph loss: 0.1936 - Test loss: 0.1761\n",
      "Epoch 10/600 - Train c_sph loss: 0.2587 - Test loss: 0.2462\n",
      "Epoch 10/600 - Train l_sph loss: 0.3134 - Test loss: 0.3033\n",
      "Epoch 10/600 - Train Acrat loss: 0.0802 - Test loss: 0.0836\n",
      "Epoch 10/600 - Train Alrat loss: 0.1256 - Test loss: 0.1221\n",
      "\n",
      "\n",
      "Epoch 11/600 - Train geom Loss: 1.092976 - Test Loss: 1.100901\n",
      "Epoch 11/600 - Train phys geom Loss: 0.400150 - Test Loss: 0.391896\n",
      "Epoch 11/600 - Train por loss: 0.1104 - Test loss: 0.1590\n",
      "Epoch 11/600 - Train sar loss: 0.0299 - Test loss: 0.0270\n",
      "Epoch 11/600 - Train sph loss: 0.1859 - Test loss: 0.1700\n",
      "Epoch 11/600 - Train c_sph loss: 0.2524 - Test loss: 0.2392\n",
      "Epoch 11/600 - Train l_sph loss: 0.3123 - Test loss: 0.3028\n",
      "Epoch 11/600 - Train Acrat loss: 0.0763 - Test loss: 0.0805\n",
      "Epoch 11/600 - Train Alrat loss: 0.1258 - Test loss: 0.1224\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20548\\2994490696.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m             \u001b[1;31m# Backward pass and optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m             \u001b[0mloss_Geom_tot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m             \u001b[1;31m#nn.utils.clip_grad_norm_(model.parameters(), 1.0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 492\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "def save_ckp(state, is_best, checkpoint_dir, best_model_dir):\n",
    "    f_path = checkpoint_dir / 'checkpoint.pt'\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath = best_model_dir / 'best_model.pt'\n",
    "        shutil.copyfile(f_path, best_fpath)\n",
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer, checkpoint['epoch']\n",
    "\n",
    "class Synthetic_geom1(torch.utils.data.Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X['images']\n",
    "        self.label = X['tmp_idx']\n",
    "        self.sph = X['sph_n']\n",
    "        self.porosity = X['por_n']\n",
    "        self.sar = X['sar_n']\n",
    "        self.length_sph = X['lsph_n']\n",
    "        self.cross_sph = X['csph_n']\n",
    "        self.class_id = X['snowflake_class_id']\n",
    "        self.A_crat = X['Acrat_n']\n",
    "        self.A_lrat = X['Alrat_n']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = (self.X[idx])\n",
    "        label = torch.tensor(self.label[idx])\n",
    "        sph = torch.tensor(self.sph[idx], dtype=torch.float32)\n",
    "        porosity = torch.tensor(self.porosity[idx], dtype=torch.float32)\n",
    "        sar = torch.tensor(self.sar[idx], dtype=torch.float32)\n",
    "        length_sph = torch.tensor(self.length_sph[idx], dtype=torch.float32)\n",
    "        cross_sph = torch.tensor(self.cross_sph[idx], dtype=torch.float32)\n",
    "        class_id = torch.tensor(self.class_id[idx], dtype=torch.float32)\n",
    "        A_crat = torch.tensor(self.A_crat[idx], dtype=torch.float32)\n",
    "        A_lrat = torch.tensor(self.A_lrat[idx], dtype=torch.float32)\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomInvert(0.5),\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5), (0.5))]) # might need to change\n",
    "        # seperate ys\n",
    "        return label, transform(X), porosity,sar,sph,cross_sph,length_sph,class_id,A_crat,A_lrat\n",
    "\n",
    "class Synthetic_geom2(torch.utils.data.Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X['images']\n",
    "        self.label = X['tmp_idx']\n",
    "        self.sph = X['sph_n']\n",
    "        self.porosity = X['por_n']\n",
    "        self.sar = X['sar_n']\n",
    "        self.length_sph = X['lsph_n']\n",
    "        self.cross_sph = X['csph_n']\n",
    "        self.class_id = X['snowflake_class_id']\n",
    "        self.A_crat = X['Acrat_n']\n",
    "        self.A_lrat = X['Alrat_n']\n",
    "        self.Df = X['Df_n']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = (self.X[idx])\n",
    "        label = torch.tensor(self.label[idx])\n",
    "        sph = torch.tensor(self.sph[idx], dtype=torch.float32)\n",
    "        porosity = torch.tensor(self.porosity[idx], dtype=torch.float32)\n",
    "        sar = torch.tensor(self.sar[idx], dtype=torch.float32)\n",
    "        length_sph = torch.tensor(self.length_sph[idx], dtype=torch.float32)\n",
    "        cross_sph = torch.tensor(self.cross_sph[idx], dtype=torch.float32)\n",
    "        class_id = torch.tensor(self.class_id[idx], dtype=torch.float32)\n",
    "        A_crat = torch.tensor(self.A_crat[idx], dtype=torch.float32)\n",
    "        A_lrat = torch.tensor(self.A_lrat[idx], dtype=torch.float32)\n",
    "        Df = torch.tensor(self.Df[idx], dtype=torch.float32)\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomInvert(0.5),\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5), (0.5))]) # might need to change\n",
    "        # seperate ys\n",
    "        return label, transform(X), porosity,sar,sph,cross_sph,length_sph,class_id,A_crat,A_lrat,Df\n",
    "\n",
    "# Train settings\n",
    "if dataset == 1: # agg\n",
    "    batch_size = 32\n",
    "    lr = 5e-5\n",
    "    num_epochs = 2000\n",
    "    if correlation == 1:\n",
    "        agg_train = Synthetic_geom1(agg_geom_train)\n",
    "        agg_test = Synthetic_geom1(agg_geom_test)\n",
    "    if correlation == 2:\n",
    "        agg_train = Synthetic_geom2(agg_geom_train)\n",
    "        agg_test = Synthetic_geom2(agg_geom_test)\n",
    "    if correlation == 3:\n",
    "        agg_train = Synthetic_geom1(agg_geom_train)\n",
    "        agg_test = Synthetic_geom1(agg_geom_test)\n",
    "    train_size = len(agg_train)\n",
    "    test_size = len(agg_test)\n",
    "    train_loader = DataLoader(agg_train, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(agg_test)\n",
    "elif dataset == 2: #plan\n",
    "    batch_size = 10\n",
    "    lr = 1e-6\n",
    "    num_epochs = 600\n",
    "    if correlation == 1:\n",
    "        full_data = Synthetic_geom1(plan_geom)\n",
    "    if correlation == 2:\n",
    "        full_data = Synthetic_geom2(plan_geom)\n",
    "    if correlation == 3:\n",
    "        full_data = Synthetic_geom1(plan_geom)\n",
    "    train_size = int(0.8 * len(full_data))\n",
    "    test_size = len(full_data) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_data, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset)\n",
    "elif dataset == 3: #graup\n",
    "    batch_size = 3\n",
    "    lr = 1e-6\n",
    "    num_epochs = 500\n",
    "    if correlation == 1:\n",
    "        full_data = Synthetic_geom1(gr_geom)\n",
    "    if correlation == 2:\n",
    "        full_data = Synthetic_geom2(gr_geom)\n",
    "    if correlation == 3:\n",
    "        full_data = Synthetic_geom1(gr_geom)\n",
    "    train_size = int(0.8 * len(full_data))\n",
    "    test_size = len(full_data) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_data, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset)\n",
    "elif dataset == 4: #col\n",
    "    batch_size = 3\n",
    "    lr = 1e-6\n",
    "    num_epochs = 500\n",
    "    if correlation == 1:\n",
    "        full_data = Synthetic_geom1(col_geom)\n",
    "    if correlation == 2:\n",
    "        full_data = Synthetic_geom2(col_geom)\n",
    "    if correlation == 3:\n",
    "        full_data = Synthetic_geom1(col_geom)\n",
    "    train_size = int(0.8 * len(full_data))\n",
    "    test_size = len(full_data) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_data, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset)\n",
    "print('Train set size:', train_size)\n",
    "print('Test set size:', test_size)\n",
    "\n",
    "class KGCNN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KGCNN1, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5),#, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 48, 3),# stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(48, 64, 3),# stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 80, 3),# stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.geom = nn.Sequential(\n",
    "            nn.Linear(80 * 14 * 14, 50), # 48*12*12 if size is 256, *4*4 128\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(50, 7), # 7 geoms return\n",
    "            nn.Softplus(),\n",
    "        )\n",
    "    def forward(self, image):\n",
    "        top_out = self.conv(image)\n",
    "        middle = top_out.view(top_out.size(0), -1)\n",
    "        geoms_out = self.geom(middle)\n",
    "        #print(image.size())\n",
    "        #print(middle1.size())\n",
    "        #print(data.size())\n",
    "        return geoms_out\n",
    "    \n",
    "class KGCNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KGCNN2, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5),#, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 48, 3),# stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(48, 64, 3),# stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 80, 3),# stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.geom = nn.Sequential(\n",
    "            nn.Linear(80 * 14 * 14, 50), # 48*12*12 if size is 256, *4*4 128\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(50, 8), # 8 geoms return\n",
    "            nn.Softplus(),\n",
    "        )\n",
    "    def forward(self, image):\n",
    "        top_out = self.conv(image)\n",
    "        middle = top_out.view(top_out.size(0), -1)\n",
    "        geoms_out = self.geom(middle)\n",
    "        #print(image.size())\n",
    "        #print(middle1.size())\n",
    "        #print(data.size())\n",
    "        return geoms_out\n",
    "\n",
    "# Custom physics loss\n",
    "def physics_loss(sar, sph, length_sph, cross_sph, A_crat, A_lrat):\n",
    "    \n",
    "    tiny = 0.000000001\n",
    "    \n",
    "    loss_c = (sph/(4*sar) - A_crat*cross_sph)**2\n",
    "    loss_l = (sph/(4*sar) - length_sph*(1/(2*sar) - A_lrat))**2\n",
    "    loss_c = torch.mean(loss_c)\n",
    "    loss_l = torch.mean(loss_l)\n",
    "    \n",
    "    # penalize geom outputs if outside range\n",
    "    if torch.max(porosity)>=1:\n",
    "        loss_por = 100*(porosity-0.9)**2\n",
    "    else:\n",
    "        loss_por = torch.tensor(0.000000001, requires_grad=True)\n",
    "        \n",
    "    if torch.max(sar) >= 0.5:\n",
    "        loss_sar = 100*(sar-0.4)**2\n",
    "    else:\n",
    "        loss_sar = torch.tensor(0.000000001, requires_grad=True)\n",
    "        \n",
    "    if torch.max(sph) >= 1:\n",
    "        loss_sph = 100*(sph-0.9)**2\n",
    "    else:\n",
    "        loss_sph = torch.tensor(0.000000001, requires_grad=True)\n",
    "\n",
    "    loss_pg = torch.mean(loss_c + loss_l + loss_por + loss_sar + loss_sph)/5\n",
    "    \n",
    "    return loss_c, loss_l, loss_pg\n",
    "\n",
    "# use GPU for computations\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(torch.cuda.is_available())\n",
    "if correlation == 1:\n",
    "    model = KGCNN1().to(device)  # Create an instance of the model\n",
    "elif correlation == 2:\n",
    "    model = KGCNN2().to(device)\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "if correlation == 1:\n",
    "    model = KGCNN1().to(device)\n",
    "    # freeze Cd layer when training\n",
    "    #    for name, param in model.named_parameters():\n",
    "    #    if param.requires_grad and 'Cd' in name:\n",
    "    #        param.requires_grad = False\n",
    "    non_frozen_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(non_frozen_parameters, lr=lr)  # Choose an optimizer\n",
    "\n",
    "    criterion = nn.MSELoss() #MSELoss()  # Mean squared error loss #L1Loss()\n",
    "\n",
    "    train_llist_Geom = []\n",
    "    train_llist_phys = []\n",
    "    train_llist_por = []\n",
    "    train_llist_sar = []\n",
    "    train_llist_sph = []\n",
    "    train_llist_c_sph = []\n",
    "    train_llist_l_sph = []\n",
    "    train_llist_Acrat = []\n",
    "    train_llist_Alrat = []\n",
    "    train_llist_pg = []\n",
    "\n",
    "    test_llist_Geom = []\n",
    "    test_llist_phys = []\n",
    "    test_llist_por = []\n",
    "    test_llist_sar = []\n",
    "    test_llist_sph = []\n",
    "    test_llist_c_sph = []\n",
    "    test_llist_l_sph = []\n",
    "    test_llist_Acrat = []\n",
    "    test_llist_Alrat = []\n",
    "    test_llist_pg = []\n",
    "\n",
    "    best_test_acc = 100\n",
    "\n",
    "    # Training loop\n",
    "    train_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model in training mode\n",
    "\n",
    "        train_Geom_loss = 0.0\n",
    "        train_por_l = 0.0\n",
    "        train_sar_l = 0.0\n",
    "        train_sph_l = 0.0\n",
    "        train_c_sph_l = 0.0\n",
    "        train_l_sph_l = 0.0\n",
    "        train_Acrat_l = 0.0\n",
    "        train_Alrat_l = 0.0\n",
    "        train_pg = 0.0\n",
    "\n",
    "        for id,images,por_dat,sar_dat,sph_dat,c_sph_dat,l_sph_dat,class_id,A_crat_dat,A_lrat_dat in train_loader:\n",
    "\n",
    "            id = id.to(device)\n",
    "            images = Variable(images, requires_grad=True)\n",
    "\n",
    "            por_dat = por_dat.view(por_dat.size(0),1)\n",
    "            sar_dat = sar_dat.view(por_dat.size(0),1)\n",
    "            sph_dat = sph_dat.view(por_dat.size(0),1)\n",
    "            c_sph_dat = c_sph_dat.view(c_sph_dat.size(0),1)\n",
    "            l_sph_dat = l_sph_dat.view(l_sph_dat.size(0),1)\n",
    "            A_crat_dat = A_crat_dat.view(A_crat_dat.size(0),1)\n",
    "            A_lrat_dat = A_lrat_dat.view(A_lrat_dat.size(0),1)\n",
    "            class_id = class_id.view(class_id.size(0),1)\n",
    "\n",
    "            images = images.to(device)\n",
    "            por_dat = por_dat.to(device)\n",
    "            sar_dat = sar_dat.to(device)\n",
    "            sph_dat = sph_dat.to(device)\n",
    "            c_sph_dat = c_sph_dat.to(device)\n",
    "            l_sph_dat = l_sph_dat.to(device)\n",
    "            A_crat_dat = A_crat_dat.to(device)\n",
    "            A_lrat_dat = A_lrat_dat.to(device)\n",
    "            class_id = class_id.to(device)\n",
    "\n",
    "            #with autograd.detect_anomaly():\n",
    "            # Forward pass\n",
    "            geoms_out = model(images)\n",
    "            sph = geoms_out[:,0]\n",
    "            cross_sph = geoms_out[:,1]\n",
    "            length_sph = geoms_out[:,2]\n",
    "            porosity = geoms_out[:,3]\n",
    "            sar = geoms_out[:,4]\n",
    "            A_crat = geoms_out[:,5]\n",
    "            A_lrat = geoms_out[:,6]\n",
    "\n",
    "            porosity = porosity.view(porosity.size(0),1)\n",
    "            sar = sar.view(sar.size(0),1)\n",
    "            sph = sph.view(sph.size(0),1)\n",
    "            cross_sph = cross_sph.view(cross_sph.size(0),1)\n",
    "            length_sph = length_sph.view(length_sph.size(0),1)\n",
    "            A_crat = A_crat.view(A_crat.size(0),1)\n",
    "            A_lrat = A_lrat.view(A_lrat.size(0),1)\n",
    "\n",
    "            #### LOSS\n",
    "            #PHYSICS GUIDED LOSS\n",
    "            loss_c,loss_l,loss_pg = physics_loss(sar*0.5,sph,length_sph*2,cross_sph*3.5,A_crat*1.5,A_lrat*1.5)\n",
    "\n",
    "            # regular geom loss, porosity and c_sph known in mascdb\n",
    "            loss_por = criterion(porosity,por_dat)\n",
    "            loss_sar = criterion(sar,sar_dat)\n",
    "            loss_sph = criterion(sph,sph_dat)\n",
    "            loss_c_sph = criterion(cross_sph,c_sph_dat)\n",
    "            loss_l_sph = criterion(length_sph,l_sph_dat)\n",
    "            loss_Acrat = criterion(A_crat,A_crat_dat)\n",
    "            loss_Alrat = criterion(A_lrat,A_lrat_dat)\n",
    "            loss_Geom = loss_por + loss_sar + loss_sph + loss_c_sph + loss_l_sph + loss_Acrat + loss_Alrat\n",
    "\n",
    "            # TOTAL GEOM LOSS\n",
    "            if custom_loss == 1:\n",
    "                loss_Geom_tot = loss_Geom + loss_pg\n",
    "            if custom_loss == 2:\n",
    "                loss_Geom_tot = loss_Geom\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss_Geom_tot.backward()\n",
    "            #nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #####\n",
    "            train_Geom_loss += loss_Geom * images.size(0)\n",
    "            train_pg += loss_pg * images.size(0)\n",
    "\n",
    "            train_por_l += loss_por * images.size(0)\n",
    "            train_sar_l += loss_sar * images.size(0)\n",
    "            train_sph_l += loss_sph * images.size(0)\n",
    "            train_c_sph_l += loss_c_sph * images.size(0)\n",
    "            train_l_sph_l += loss_l_sph * images.size(0)\n",
    "            train_Acrat_l += loss_Acrat * images.size(0)\n",
    "            train_Alrat_l += loss_Alrat * images.size(0)\n",
    "\n",
    "        train_Geom_loss /= len(train_loader.dataset)\n",
    "        train_por_l /= len(train_loader.dataset)\n",
    "        train_sar_l /= len(train_loader.dataset)\n",
    "        train_sph_l /= len(train_loader.dataset)\n",
    "        train_c_sph_l /= len(train_loader.dataset)\n",
    "        train_l_sph_l /= len(train_loader.dataset)\n",
    "        train_Acrat_l /= len(train_loader.dataset)\n",
    "        train_Alrat_l /= len(train_loader.dataset)\n",
    "        train_pg /= len(train_loader.dataset)\n",
    "\n",
    "        train_llist_Geom.append(train_Geom_loss.item())\n",
    "        train_llist_por.append(train_por_l.item())\n",
    "        train_llist_sar.append(train_sar_l.item())\n",
    "        train_llist_sph.append(train_sph_l.item())\n",
    "        train_llist_c_sph.append(train_c_sph_l.item())\n",
    "        train_llist_l_sph.append(train_l_sph_l.item())\n",
    "        train_llist_Acrat.append(train_Acrat_l.item())\n",
    "        train_llist_Alrat.append(train_Alrat_l.item())\n",
    "        train_llist_pg.append(train_pg.item())\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()  # Set the model in evaluation mode\n",
    "\n",
    "        test_Geom_loss = 0.0\n",
    "        test_por_l = 0.0\n",
    "        test_sar_l = 0.0\n",
    "        test_sph_l = 0.0\n",
    "        test_c_sph_l = 0.0\n",
    "        test_l_sph_l = 0.0\n",
    "        test_Acrat_l = 0.0\n",
    "        test_Alrat_l = 0.0\n",
    "        test_pg = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for id,images,por_dat,sar_dat,sph_dat,c_sph_dat,l_sph_dat,class_id,A_crat_dat,A_lrat_dat in test_loader:\n",
    "\n",
    "                id = id.to(device)\n",
    "                images = Variable(images, requires_grad=True)\n",
    "\n",
    "                por_dat = por_dat.view(por_dat.size(0),1)\n",
    "                sar_dat = sar_dat.view(por_dat.size(0),1)\n",
    "                sph_dat = sph_dat.view(por_dat.size(0),1)\n",
    "                c_sph_dat = c_sph_dat.view(c_sph_dat.size(0),1)\n",
    "                l_sph_dat = l_sph_dat.view(l_sph_dat.size(0),1)\n",
    "                A_crat_dat = A_crat_dat.view(A_crat_dat.size(0),1)\n",
    "                A_lrat_dat = A_lrat_dat.view(A_lrat_dat.size(0),1)\n",
    "                class_id = class_id.view(class_id.size(0),1)\n",
    "\n",
    "                images = images.to(device)\n",
    "                por_dat = por_dat.to(device)\n",
    "                sar_dat = sar_dat.to(device)\n",
    "                sph_dat = sph_dat.to(device)\n",
    "                c_sph_dat = c_sph_dat.to(device)\n",
    "                l_sph_dat = l_sph_dat.to(device)\n",
    "                A_crat_dat = A_crat_dat.to(device)\n",
    "                A_lrat_dat = A_lrat_dat.to(device)\n",
    "                class_id = class_id.to(device)\n",
    "\n",
    "                #with autograd.detect_anomaly():\n",
    "                # Forward pass\n",
    "                geoms_out = model(images)\n",
    "                sph = geoms_out[:,0]\n",
    "                cross_sph = geoms_out[:,1]\n",
    "                length_sph = geoms_out[:,2]\n",
    "                porosity = geoms_out[:,3]\n",
    "                sar = geoms_out[:,4]\n",
    "                A_crat = geoms_out[:,5]\n",
    "                A_lrat = geoms_out[:,6]\n",
    "\n",
    "                porosity = porosity.view(porosity.size(0),1)\n",
    "                sar = sar.view(sar.size(0),1)\n",
    "                sph = sph.view(sph.size(0),1)\n",
    "                cross_sph = cross_sph.view(cross_sph.size(0),1)\n",
    "                length_sph = length_sph.view(length_sph.size(0),1)\n",
    "                A_crat = A_crat.view(A_crat.size(0),1)\n",
    "                A_lrat = A_lrat.view(A_lrat.size(0),1)\n",
    "\n",
    "                #### LOSS\n",
    "                #PHYSICS GUIDED LOSS\n",
    "                loss_c,loss_l,loss_pg = physics_loss(sar*0.5,sph,length_sph*2,cross_sph*3.5,A_crat*1.5,A_lrat*1.5)\n",
    "\n",
    "                # regular geom loss, porosity and c_sph known in mascdb\n",
    "                loss_por = criterion(porosity,por_dat)\n",
    "                loss_sar = criterion(sar,sar_dat)\n",
    "                loss_sph = criterion(sph,sph_dat)\n",
    "                loss_c_sph = criterion(cross_sph,c_sph_dat)\n",
    "                loss_l_sph = criterion(length_sph,l_sph_dat)\n",
    "                loss_Acrat = criterion(A_crat,A_crat_dat)\n",
    "                loss_Alrat = criterion(A_lrat,A_lrat_dat)\n",
    "                loss_Geom = loss_por + loss_sar + loss_sph + loss_c_sph + loss_l_sph + loss_Acrat + loss_Alrat\n",
    "\n",
    "                # TOTAL GEOM LOSS\n",
    "                if custom_loss == 1:\n",
    "                    loss_Geom_tot = loss_Geom + loss_pg\n",
    "                if custom_loss == 2:\n",
    "                    loss_Geom_tot = loss_Geom\n",
    "\n",
    "                # TOTAL loss w/o phys Cd\n",
    "                # sum physics guided and regular losses from Cd and geom parameters\n",
    "\n",
    "                test_Geom_loss += loss_Geom * images.size(0)\n",
    "                test_pg += loss_pg * images.size(0)\n",
    "\n",
    "                test_por_l += loss_por * images.size(0)\n",
    "                test_sar_l += loss_sar * images.size(0)\n",
    "                test_sph_l += loss_sph * images.size(0)\n",
    "                test_c_sph_l += loss_c_sph * images.size(0)\n",
    "                test_l_sph_l += loss_l_sph * images.size(0)\n",
    "                test_Acrat_l += loss_Acrat * images.size(0)\n",
    "                test_Alrat_l += loss_Alrat * images.size(0)\n",
    "\n",
    "        test_Geom_loss /= len(test_loader.dataset)\n",
    "        test_por_l /= len(test_loader.dataset)\n",
    "        test_sar_l /= len(test_loader.dataset)\n",
    "        test_sph_l /= len(test_loader.dataset)\n",
    "        test_c_sph_l /= len(test_loader.dataset)\n",
    "        test_l_sph_l /= len(test_loader.dataset)\n",
    "        test_Acrat_l /= len(test_loader.dataset)\n",
    "        test_Alrat_l /= len(test_loader.dataset)\n",
    "        test_pg /= len(test_loader.dataset)\n",
    "\n",
    "        test_llist_Geom.append(test_Geom_loss.item())\n",
    "        test_llist_por.append(test_por_l.item())\n",
    "        test_llist_sar.append(test_sar_l.item())\n",
    "        test_llist_sph.append(test_sph_l.item())\n",
    "        test_llist_c_sph.append(test_c_sph_l.item())\n",
    "        test_llist_l_sph.append(test_l_sph_l.item())\n",
    "        test_llist_Acrat.append(test_Acrat_l.item())\n",
    "        test_llist_Alrat.append(test_Alrat_l.item())\n",
    "        test_llist_pg.append(test_pg.item())\n",
    "\n",
    "        if test_Geom_loss < best_test_acc:\n",
    "            checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "            torch.save(checkpoint, dir_model)\n",
    "            goodplanmodel_epoch = epoch\n",
    "            best_test_acc = test_Geom_loss\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train geom Loss: {train_Geom_loss:.6f} - Test Loss: {test_Geom_loss:.6f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train phys geom Loss: {train_pg:.6f} - Test Loss: {test_pg:.6f}\")\n",
    "        #print(f\"Epoch {epoch+1}/{num_epochs} - Train phys Cd Loss: {train_p_Cd_loss:.4f} - Test Loss: {test_p_Cd_loss:.4f}\")\n",
    "        #print(f\"Epoch {epoch+1}/{num_epochs} - Train TOTAL Loss: {train_loss_TOTAL:.4f} - Test Loss: {test_loss_TOTAL:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train por loss: {train_por_l:.4f} - Test loss: {test_por_l:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train sar loss: {train_sar_l:.4f} - Test loss: {test_sar_l:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train sph loss: {train_sph_l:.4f} - Test loss: {test_sph_l:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train c_sph loss: {train_c_sph_l:.4f} - Test loss: {test_c_sph_l:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train l_sph loss: {train_l_sph_l:.4f} - Test loss: {test_l_sph_l:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Acrat loss: {train_Acrat_l:.4f} - Test loss: {test_Acrat_l:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Alrat loss: {train_Alrat_l:.4f} - Test loss: {test_Alrat_l:.4f}\")\n",
    "        print('\\n')\n",
    "\n",
    "if correlation == 2:\n",
    "    model = KGCNN2().to(device)\n",
    "    # freeze Cd layer when training purely for geoms\n",
    "    #for name, param in model.named_parameters():\n",
    "    #    if param.requires_grad and 'Cd' in name:\n",
    "    #        param.requires_grad = False\n",
    "    non_frozen_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(non_frozen_parameters, lr=lr)  # Choose an optimizer\n",
    "\n",
    "    criterion = nn.MSELoss() #MSELoss()  # Mean squared error loss #L1Loss()\n",
    "\n",
    "    train_llist_Geom = []\n",
    "    train_llist_phys = []\n",
    "    train_llist_por = []\n",
    "    train_llist_sar = []\n",
    "    train_llist_sph = []\n",
    "    train_llist_c_sph = []\n",
    "    train_llist_l_sph = []\n",
    "    train_llist_Acrat = []\n",
    "    train_llist_Alrat = []\n",
    "    train_llist_Df = []\n",
    "    train_llist_pg = []\n",
    "\n",
    "    test_llist_Geom = []\n",
    "    test_llist_phys = []\n",
    "    test_llist_por = []\n",
    "    test_llist_sar = []\n",
    "    test_llist_sph = []\n",
    "    test_llist_c_sph = []\n",
    "    test_llist_l_sph = []\n",
    "    test_llist_Acrat = []\n",
    "    test_llist_Alrat = []\n",
    "    test_llist_Df = []\n",
    "    test_llist_pg = []\n",
    "\n",
    "    best_test_acc = 100\n",
    "\n",
    "    # Training loop\n",
    "    train_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model in training mode\n",
    "\n",
    "        train_Geom_loss = 0.0\n",
    "        train_por_l = 0.0\n",
    "        train_sar_l = 0.0\n",
    "        train_sph_l = 0.0\n",
    "        train_c_sph_l = 0.0\n",
    "        train_l_sph_l = 0.0\n",
    "        train_Acrat_l = 0.0\n",
    "        train_Alrat_l = 0.0\n",
    "        train_Df_l = 0.0\n",
    "        train_pg = 0.0\n",
    "\n",
    "        for id,images,por_dat,sar_dat,sph_dat,c_sph_dat,l_sph_dat,class_id,A_crat_dat,A_lrat_dat,Df_dat in train_loader:\n",
    "\n",
    "            id = id.to(device)\n",
    "            images = Variable(images, requires_grad=True)\n",
    "\n",
    "            por_dat = por_dat.view(por_dat.size(0),1)\n",
    "            sar_dat = sar_dat.view(por_dat.size(0),1)\n",
    "            sph_dat = sph_dat.view(por_dat.size(0),1)\n",
    "            c_sph_dat = c_sph_dat.view(c_sph_dat.size(0),1)\n",
    "            l_sph_dat = l_sph_dat.view(l_sph_dat.size(0),1)\n",
    "            A_crat_dat = A_crat_dat.view(A_crat_dat.size(0),1)\n",
    "            A_lrat_dat = A_lrat_dat.view(A_lrat_dat.size(0),1)\n",
    "            class_id = class_id.view(class_id.size(0),1)\n",
    "            Df_dat = Df_dat.view(Df_dat.size(0),1)\n",
    "\n",
    "            images = images.to(device)\n",
    "            por_dat = por_dat.to(device)\n",
    "            sar_dat = sar_dat.to(device)\n",
    "            sph_dat = sph_dat.to(device)\n",
    "            c_sph_dat = c_sph_dat.to(device)\n",
    "            l_sph_dat = l_sph_dat.to(device)\n",
    "            A_crat_dat = A_crat_dat.to(device)\n",
    "            A_lrat_dat = A_lrat_dat.to(device)\n",
    "            class_id = class_id.to(device)\n",
    "            Df_dat = Df_dat.to(device)\n",
    "\n",
    "            #with autograd.detect_anomaly():\n",
    "            # Forward pass\n",
    "            geoms_out = model(images)\n",
    "            sph = geoms_out[:,0]\n",
    "            cross_sph = geoms_out[:,1]\n",
    "            length_sph = geoms_out[:,2]\n",
    "            porosity = geoms_out[:,3]\n",
    "            sar = geoms_out[:,4]\n",
    "            A_crat = geoms_out[:,5]\n",
    "            A_lrat = geoms_out[:,6]\n",
    "            Df = geoms_out[:,7]\n",
    "\n",
    "            porosity = porosity.view(porosity.size(0),1)\n",
    "            sar = sar.view(sar.size(0),1)\n",
    "            sph = sph.view(sph.size(0),1)\n",
    "            cross_sph = cross_sph.view(cross_sph.size(0),1)\n",
    "            length_sph = length_sph.view(length_sph.size(0),1)\n",
    "            A_crat = A_crat.view(A_crat.size(0),1)\n",
    "            A_lrat = A_lrat.view(A_lrat.size(0),1)\n",
    "            Df = Df.view(Df.size(0),1)\n",
    "\n",
    "            #### LOSS\n",
    "            #PHYSICS GUIDED LOSS\n",
    "            loss_c,loss_l,loss_pg = physics_loss(sar*0.5,sph,length_sph*2,cross_sph*3.5,A_crat*1.5,A_lrat*1.5)\n",
    "\n",
    "            # regular geom loss, porosity and c_sph known in mascdb\n",
    "            loss_por = criterion(porosity,por_dat)\n",
    "            loss_sar = criterion(sar,sar_dat)\n",
    "            loss_sph = criterion(sph,sph_dat)\n",
    "            loss_c_sph = criterion(cross_sph,c_sph_dat)\n",
    "            loss_l_sph = criterion(length_sph,l_sph_dat)\n",
    "            loss_Acrat = criterion(A_crat,A_crat_dat)\n",
    "            loss_Alrat = criterion(A_lrat,A_lrat_dat)\n",
    "            loss_Df = criterion(Df,Df_dat)\n",
    "            loss_Geom = loss_por + loss_sar + loss_sph + loss_c_sph + loss_l_sph + loss_Acrat + loss_Alrat + loss_Df\n",
    "\n",
    "            # TOTAL GEOM LOSS\n",
    "            if custom_loss == 1:\n",
    "                loss_Geom_tot = loss_Geom + loss_pg\n",
    "            if custom_loss == 2:\n",
    "                loss_Geom_tot = loss_Geom\n",
    "\n",
    "            # TOTAL loss w/o phys Cd\n",
    "            # sum physics guided and regular losses from Cd and geom parameters\n",
    "            #loss_TOTAL = loss_Geom_tot + loss_Cd + Cd_bias**2\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss_Geom_tot.backward()\n",
    "            #nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #####\n",
    "            train_Geom_loss += loss_Geom * images.size(0)\n",
    "            train_pg += loss_pg * images.size(0)\n",
    "\n",
    "            train_por_l += loss_por * images.size(0)\n",
    "            train_sar_l += loss_sar * images.size(0)\n",
    "            train_sph_l += loss_sph * images.size(0)\n",
    "            train_c_sph_l += loss_c_sph * images.size(0)\n",
    "            train_l_sph_l += loss_l_sph * images.size(0)\n",
    "            train_Acrat_l += loss_Acrat * images.size(0)\n",
    "            train_Alrat_l += loss_Alrat * images.size(0)\n",
    "            train_Df_l += loss_Df * images.size(0)\n",
    "\n",
    "        train_Geom_loss /= len(train_loader.dataset)\n",
    "        train_por_l /= len(train_loader.dataset)\n",
    "        train_sar_l /= len(train_loader.dataset)\n",
    "        train_sph_l /= len(train_loader.dataset)\n",
    "        train_c_sph_l /= len(train_loader.dataset)\n",
    "        train_l_sph_l /= len(train_loader.dataset)\n",
    "        train_Acrat_l /= len(train_loader.dataset)\n",
    "        train_Alrat_l /= len(train_loader.dataset)\n",
    "        train_Df_l /= len(train_loader.dataset)\n",
    "        train_pg /= len(train_loader.dataset)\n",
    "\n",
    "        train_llist_Geom.append(train_Geom_loss.item())\n",
    "        train_llist_por.append(train_por_l.item())\n",
    "        train_llist_sar.append(train_sar_l.item())\n",
    "        train_llist_sph.append(train_sph_l.item())\n",
    "        train_llist_c_sph.append(train_c_sph_l.item())\n",
    "        train_llist_l_sph.append(train_l_sph_l.item())\n",
    "        train_llist_Acrat.append(train_Acrat_l.item())\n",
    "        train_llist_Alrat.append(train_Alrat_l.item())\n",
    "        train_llist_Df.append(train_Df_l.item())\n",
    "        train_llist_pg.append(train_pg.item())\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()  # Set the model in evaluation mode\n",
    "\n",
    "        test_Geom_loss = 0.0\n",
    "        test_por_l = 0.0\n",
    "        test_sar_l = 0.0\n",
    "        test_sph_l = 0.0\n",
    "        test_c_sph_l = 0.0\n",
    "        test_l_sph_l = 0.0\n",
    "        test_Acrat_l = 0.0\n",
    "        test_Alrat_l = 0.0\n",
    "        test_Df_l = 0.0\n",
    "        test_pg = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for id,images,por_dat,sar_dat,sph_dat,c_sph_dat,l_sph_dat,class_id,A_crat_dat,A_lrat_dat,Df_dat in test_loader:\n",
    "\n",
    "                id = id.to(device)\n",
    "                images = Variable(images, requires_grad=True)\n",
    "\n",
    "                por_dat = por_dat.view(por_dat.size(0),1)\n",
    "                sar_dat = sar_dat.view(por_dat.size(0),1)\n",
    "                sph_dat = sph_dat.view(por_dat.size(0),1)\n",
    "                c_sph_dat = c_sph_dat.view(c_sph_dat.size(0),1)\n",
    "                l_sph_dat = l_sph_dat.view(l_sph_dat.size(0),1)\n",
    "                A_crat_dat = A_crat_dat.view(A_crat_dat.size(0),1)\n",
    "                A_lrat_dat = A_lrat_dat.view(A_lrat_dat.size(0),1)\n",
    "                class_id = class_id.view(class_id.size(0),1)\n",
    "                Df_dat = Df_dat.view(Df_dat.size(0),1)\n",
    "\n",
    "                images = images.to(device)\n",
    "                por_dat = por_dat.to(device)\n",
    "                sar_dat = sar_dat.to(device)\n",
    "                sph_dat = sph_dat.to(device)\n",
    "                c_sph_dat = c_sph_dat.to(device)\n",
    "                l_sph_dat = l_sph_dat.to(device)\n",
    "                A_crat_dat = A_crat_dat.to(device)\n",
    "                A_lrat_dat = A_lrat_dat.to(device)\n",
    "                class_id = class_id.to(device)\n",
    "                Df_dat = Df_dat.to(device)\n",
    "\n",
    "                #with autograd.detect_anomaly():\n",
    "                # Forward pass\n",
    "                geoms_out = model(images)\n",
    "                sph = geoms_out[:,0]\n",
    "                cross_sph = geoms_out[:,1]\n",
    "                length_sph = geoms_out[:,2]\n",
    "                porosity = geoms_out[:,3]\n",
    "                sar = geoms_out[:,4]\n",
    "                A_crat = geoms_out[:,5]\n",
    "                A_lrat = geoms_out[:,6]\n",
    "                Df = geoms_out[:,7]\n",
    "\n",
    "                porosity = porosity.view(porosity.size(0),1)\n",
    "                sar = sar.view(sar.size(0),1)\n",
    "                sph = sph.view(sph.size(0),1)\n",
    "                cross_sph = cross_sph.view(cross_sph.size(0),1)\n",
    "                length_sph = length_sph.view(length_sph.size(0),1)\n",
    "                A_crat = A_crat.view(A_crat.size(0),1)\n",
    "                A_lrat = A_lrat.view(A_lrat.size(0),1)\n",
    "                Df = Df.view(Df.size(0),1)\n",
    "\n",
    "                #### LOSS\n",
    "                #PHYSICS GUIDED LOSS\n",
    "                loss_c,loss_l,loss_pg = physics_loss(sar*0.5,sph,length_sph*2,cross_sph*3.5,A_crat*1.5,A_lrat*1.5)\n",
    "\n",
    "                # regular geom loss, porosity and c_sph known in mascdb\n",
    "                loss_por = criterion(porosity,por_dat)\n",
    "                loss_sar = criterion(sar,sar_dat)\n",
    "                loss_sph = criterion(sph,sph_dat)\n",
    "                loss_c_sph = criterion(cross_sph,c_sph_dat)\n",
    "                loss_l_sph = criterion(length_sph,l_sph_dat)\n",
    "                loss_Acrat = criterion(A_crat,A_crat_dat)\n",
    "                loss_Alrat = criterion(A_lrat,A_lrat_dat)\n",
    "                loss_Df = criterion(Df,Df_dat)\n",
    "                loss_Geom = loss_por + loss_sar + loss_sph + loss_c_sph + loss_l_sph + loss_Acrat + loss_Alrat + loss_Df\n",
    "\n",
    "                # TOTAL GEOM LOSS\n",
    "                if custom_loss == 1:\n",
    "                    loss_Geom_tot = loss_Geom + loss_pg\n",
    "                if custom_loss == 2:\n",
    "                    loss_Geom_tot = loss_Geom\n",
    "\n",
    "                # TOTAL loss w/o phys Cd\n",
    "                # sum physics guided and regular losses from Cd and geom parameters\n",
    "\n",
    "                test_Geom_loss += loss_Geom * images.size(0)\n",
    "                test_pg += loss_pg * images.size(0)\n",
    "\n",
    "                test_por_l += loss_por * images.size(0)\n",
    "                test_sar_l += loss_sar * images.size(0)\n",
    "                test_sph_l += loss_sph * images.size(0)\n",
    "                test_c_sph_l += loss_c_sph * images.size(0)\n",
    "                test_l_sph_l += loss_l_sph * images.size(0)\n",
    "                test_Acrat_l += loss_Acrat * images.size(0)\n",
    "                test_Alrat_l += loss_Alrat * images.size(0)\n",
    "\n",
    "        test_Geom_loss /= len(test_loader.dataset)\n",
    "        test_por_l /= len(test_loader.dataset)\n",
    "        test_sar_l /= len(test_loader.dataset)\n",
    "        test_sph_l /= len(test_loader.dataset)\n",
    "        test_c_sph_l /= len(test_loader.dataset)\n",
    "        test_l_sph_l /= len(test_loader.dataset)\n",
    "        test_Acrat_l /= len(test_loader.dataset)\n",
    "        test_Alrat_l /= len(test_loader.dataset)\n",
    "        test_Df_l += loss_Df * images.size(0)\n",
    "        test_pg /= len(test_loader.dataset)\n",
    "\n",
    "        test_llist_Geom.append(test_Geom_loss.item())\n",
    "        test_llist_por.append(test_por_l.item())\n",
    "        test_llist_sar.append(test_sar_l.item())\n",
    "        test_llist_sph.append(test_sph_l.item())\n",
    "        test_llist_c_sph.append(test_c_sph_l.item())\n",
    "        test_llist_l_sph.append(test_l_sph_l.item())\n",
    "        test_llist_Acrat.append(test_Acrat_l.item())\n",
    "        test_llist_Alrat.append(test_Alrat_l.item())\n",
    "        test_llist_Df.append(test_Df_l.item())\n",
    "        test_llist_pg.append(test_pg.item())\n",
    "\n",
    "        if test_Geom_loss < best_test_acc:\n",
    "            checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "            torch.save(checkpoint, dir_model)\n",
    "            goodplanmodel_epoch = epoch\n",
    "            best_test_acc = test_Geom_loss\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train geom Loss: {train_Geom_loss:.6f} - Test Loss: {test_Geom_loss:.6f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train phys geom Loss: {train_pg:.6f} - Test Loss: {test_pg:.6f}\")\n",
    "        #print(f\"Epoch {epoch+1}/{num_epochs} - Train phys Cd Loss: {train_p_Cd_loss:.4f} - Test Loss: {test_p_Cd_loss:.4f}\")\n",
    "        #print(f\"Epoch {epoch+1}/{num_epochs} - Train TOTAL Loss: {train_loss_TOTAL:.4f} - Test Loss: {test_loss_TOTAL:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train por loss: {train_por_l:.4f} - Test loss: {test_por_l:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train sar loss: {train_sar_l:.4f} - Test loss: {test_sar_l:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train sph loss: {train_sph_l:.4f} - Test loss: {test_sph_l:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train c_sph loss: {train_c_sph_l:.4f} - Test loss: {test_c_sph_l:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train l_sph loss: {train_l_sph_l:.4f} - Test loss: {test_l_sph_l:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Acrat loss: {train_Acrat_l:.4f} - Test loss: {test_Acrat_l:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Alrat loss: {train_Alrat_l:.4f} - Test loss: {test_Alrat_l:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Df loss: {train_Df_l:.4f} - Test loss: {test_Df_l:.4f}\")\n",
    "        print('\\n')\n",
    "        \n",
    "print('Completed training. Best model saved at epoch ',goodplanmodel_epoch)\n",
    "print('Now evaluating performance')\n",
    "model.load_state_dict(torch.load(dir_model))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ckp_path = dir_model\n",
    "non_frozen_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(non_frozen_parameters, lr=lr)\n",
    "model, optimizer, start_epoch = load_ckp(ckp_path, model, optimizer)\n",
    "\n",
    "if correlation == 1:\n",
    "    model.eval()  # Set the model in evaluation mode\n",
    "    criterion = nn.MSELoss() #MSELoss()  # Mean squared error loss #L1Loss()\n",
    "    outs = []\n",
    "    por_out = []\n",
    "    sar_out = []\n",
    "    c_sph_out = []\n",
    "    l_sph_out = []\n",
    "    sph_out = []\n",
    "    Acrat_out = []\n",
    "    Alrat_out = []\n",
    "\n",
    "    answers = []\n",
    "    por_ans = []\n",
    "    sar_ans = []\n",
    "    c_sph_ans = []\n",
    "    l_sph_ans = []\n",
    "    sph_ans = []\n",
    "    Acrat_ans = []\n",
    "    Alrat_ans = []\n",
    "\n",
    "    test_Geom_loss = 0.0\n",
    "    test_p_Geom_loss = 0.0\n",
    "\n",
    "    test_ids = []\n",
    "    with torch.no_grad():\n",
    "        for id,images,por_dat,sar_dat,sph_dat,c_sph_dat,l_sph_dat,class_id,A_crat_dat,A_lrat_dat in test_loader:\n",
    "\n",
    "            id = id.to(device)\n",
    "            images = Variable(images, requires_grad=True)\n",
    "\n",
    "            por_dat = por_dat.view(por_dat.size(0),1)\n",
    "            sar_dat = sar_dat.view(por_dat.size(0),1)\n",
    "            sph_dat = sph_dat.view(por_dat.size(0),1)\n",
    "            c_sph_dat = c_sph_dat.view(c_sph_dat.size(0),1)\n",
    "            l_sph_dat = l_sph_dat.view(l_sph_dat.size(0),1)\n",
    "            A_crat_dat = A_crat_dat.view(A_crat_dat.size(0),1)\n",
    "            A_lrat_dat = A_lrat_dat.view(A_lrat_dat.size(0),1)\n",
    "            class_id = class_id.view(class_id.size(0),1)\n",
    "\n",
    "            images = images.to(device)\n",
    "            por_dat = por_dat.to(device)\n",
    "            sar_dat = sar_dat.to(device)\n",
    "            sph_dat = sph_dat.to(device)\n",
    "            c_sph_dat = c_sph_dat.to(device)\n",
    "            l_sph_dat = l_sph_dat.to(device)\n",
    "            A_crat_dat = A_crat_dat.to(device)\n",
    "            A_lrat_dat = A_lrat_dat.to(device)\n",
    "            class_id = class_id.to(device)\n",
    "\n",
    "            #with autograd.detect_anomaly():\n",
    "            # Forward pass\n",
    "            geoms_out = model(images)\n",
    "            sph = geoms_out[:,0]\n",
    "            cross_sph = geoms_out[:,1]\n",
    "            length_sph = geoms_out[:,2]\n",
    "            porosity = geoms_out[:,3]\n",
    "            sar = geoms_out[:,4]\n",
    "            A_crat = geoms_out[:,5]\n",
    "            A_lrat = geoms_out[:,6]\n",
    "\n",
    "            porosity = porosity.view(porosity.size(0),1)\n",
    "            sar = sar.view(sar.size(0),1)\n",
    "            sph = sph.view(sph.size(0),1)\n",
    "            cross_sph = cross_sph.view(cross_sph.size(0),1)\n",
    "            length_sph = length_sph.view(length_sph.size(0),1)\n",
    "            A_crat = A_crat.view(A_crat.size(0),1)\n",
    "            A_lrat = A_lrat.view(A_lrat.size(0),1)\n",
    "\n",
    "            #### LOSS\n",
    "            #PHYSICS GUIDED LOSS\n",
    "            #loss_p_Cd,loss_pg1,loss_pg2,loss_pg3,loss_pg = physics_loss(Cd_inv,porosity,sar,sph,length_sph,cross_sph,A_char,A_cross,A_length,rho_air,fallspeed,V_CH)\n",
    "            #loss_p_Cd = loss_p_Cd.to(device)\n",
    "            #loss_pg = loss_pg.to(device)\n",
    "            loss_pg = 0.0\n",
    "            # regular geom loss, porosity and c_sph known in mascdb\n",
    "            loss_por = criterion(porosity,por_dat)\n",
    "            loss_sar = criterion(sar,sar_dat)\n",
    "            loss_sph = criterion(sph,sph_dat)\n",
    "            loss_c_sph = criterion(cross_sph,c_sph_dat)\n",
    "            loss_l_sph = criterion(length_sph,l_sph_dat)\n",
    "            loss_Acrat = criterion(A_crat,A_crat_dat)\n",
    "            loss_Alrat = criterion(A_lrat,A_lrat_dat)\n",
    "            loss_Geom = loss_por + loss_sar + loss_sph + loss_c_sph + loss_l_sph + loss_Acrat + loss_Alrat\n",
    "\n",
    "            test_Geom_loss += loss_Geom * images.size(0)\n",
    "            test_p_Geom_loss += loss_pg * images.size(0)\n",
    "\n",
    "            sph_out.append(sph.item())\n",
    "            c_sph_out.append(cross_sph.item())\n",
    "            l_sph_out.append(length_sph.item())\n",
    "            por_out.append(porosity.item())\n",
    "            sar_out.append(sar.item())\n",
    "            Acrat_out.append(A_crat.item())\n",
    "            Alrat_out.append(A_lrat.item())\n",
    "\n",
    "            por_ans.append(por_dat.item())\n",
    "            sar_ans.append(sar_dat.item())\n",
    "            sph_ans.append(sph_dat.item())\n",
    "            c_sph_ans.append(c_sph_dat.item())\n",
    "            l_sph_ans.append(l_sph_dat.item())\n",
    "            Acrat_ans.append(A_crat_dat.item())\n",
    "            Alrat_ans.append(A_lrat_dat.item())\n",
    "\n",
    "            test_ids.append(id.item())\n",
    "            #err += (abs(outputs - t_Cd)/t_Cd)*100\n",
    "            #print(err)\n",
    "            #loss =  l + loss_mean\n",
    "    #outs_act = np.array(outs)\n",
    "    #answers_act = np.array(answers)\n",
    "    por_out = np.array(por_out)\n",
    "    sar_out = np.array(sar_out)\n",
    "    sph_out = np.array(sph_out)\n",
    "    l_sph_out = np.array(l_sph_out)\n",
    "    c_sph_out = np.array(c_sph_out)\n",
    "    Acrat_out = np.array(Acrat_out)\n",
    "    Alrat_out = np.array(Alrat_out)\n",
    "    por_out = por_out\n",
    "    sar_out = sar_out*0.5\n",
    "    sph_out = sph_out\n",
    "    l_sph_out = l_sph_out*2\n",
    "    c_sph_out = c_sph_out*3.5\n",
    "    Acrat_out = Acrat_out*1.5\n",
    "    Alrat_out = Alrat_out*1.5\n",
    "\n",
    "    por_ans = np.array(por_ans)\n",
    "    sar_ans = np.array(sar_ans)\n",
    "    sph_ans = np.array(sph_ans)\n",
    "    l_sph_ans = np.array(l_sph_ans)\n",
    "    c_sph_ans = np.array(c_sph_ans)\n",
    "    Acrat_ans = np.array(Acrat_ans)\n",
    "    Alrat_ans = np.array(Alrat_ans)\n",
    "    por_ans = por_ans\n",
    "    sar_ans = sar_ans*0.5\n",
    "    sph_ans = sph_ans\n",
    "    l_sph_ans = l_sph_ans*2\n",
    "    c_sph_ans = c_sph_ans*3.5\n",
    "    Acrat_ans = Acrat_ans*1.5\n",
    "    Alrat_ans = Alrat_ans*1.5\n",
    "\n",
    "    test_ids = np.array(test_ids)\n",
    "    \n",
    "if correlation == 2:\n",
    "    model.eval()  # Set the model in evaluation mode\n",
    "    criterion = nn.MSELoss() #MSELoss()  # Mean squared error loss #L1Loss()\n",
    "    outs = []\n",
    "    por_out = []\n",
    "    sar_out = []\n",
    "    c_sph_out = []\n",
    "    l_sph_out = []\n",
    "    sph_out = []\n",
    "    Acrat_out = []\n",
    "    Alrat_out = []\n",
    "    Df_out = []\n",
    "\n",
    "    answers = []\n",
    "    por_ans = []\n",
    "    sar_ans = []\n",
    "    c_sph_ans = []\n",
    "    l_sph_ans = []\n",
    "    sph_ans = []\n",
    "    Acrat_ans = []\n",
    "    Alrat_ans = []\n",
    "    Df_ans = []\n",
    "\n",
    "    test_Geom_loss = 0.0\n",
    "    test_p_Geom_loss = 0.0\n",
    "\n",
    "    test_ids = []\n",
    "    with torch.no_grad():\n",
    "        for id,images,por_dat,sar_dat,sph_dat,c_sph_dat,l_sph_dat,class_id,A_crat_dat,A_lrat_dat,Df_dat in test_loader:\n",
    "\n",
    "            id = id.to(device)\n",
    "            images = Variable(images, requires_grad=True)\n",
    "\n",
    "            por_dat = por_dat.view(por_dat.size(0),1)\n",
    "            sar_dat = sar_dat.view(por_dat.size(0),1)\n",
    "            sph_dat = sph_dat.view(por_dat.size(0),1)\n",
    "            c_sph_dat = c_sph_dat.view(c_sph_dat.size(0),1)\n",
    "            l_sph_dat = l_sph_dat.view(l_sph_dat.size(0),1)\n",
    "            A_crat_dat = A_crat_dat.view(A_crat_dat.size(0),1)\n",
    "            A_lrat_dat = A_lrat_dat.view(A_lrat_dat.size(0),1)\n",
    "            class_id = class_id.view(class_id.size(0),1)\n",
    "            Df_dat = Df_dat.view(Df_dat.size(0),1)\n",
    "\n",
    "            images = images.to(device)\n",
    "            por_dat = por_dat.to(device)\n",
    "            sar_dat = sar_dat.to(device)\n",
    "            sph_dat = sph_dat.to(device)\n",
    "            c_sph_dat = c_sph_dat.to(device)\n",
    "            l_sph_dat = l_sph_dat.to(device)\n",
    "            A_crat_dat = A_crat_dat.to(device)\n",
    "            A_lrat_dat = A_lrat_dat.to(device)\n",
    "            class_id = class_id.to(device)\n",
    "            Df_dat = Df_dat.to(device)\n",
    "\n",
    "            #with autograd.detect_anomaly():\n",
    "            # Forward pass\n",
    "            geoms_out = model(images)\n",
    "            sph = geoms_out[:,0]\n",
    "            cross_sph = geoms_out[:,1]\n",
    "            length_sph = geoms_out[:,2]\n",
    "            porosity = geoms_out[:,3]\n",
    "            sar = geoms_out[:,4]\n",
    "            A_crat = geoms_out[:,5]\n",
    "            A_lrat = geoms_out[:,6]\n",
    "            Df = geoms_out[:,7]\n",
    "\n",
    "            porosity = porosity.view(porosity.size(0),1)\n",
    "            sar = sar.view(sar.size(0),1)\n",
    "            sph = sph.view(sph.size(0),1)\n",
    "            cross_sph = cross_sph.view(cross_sph.size(0),1)\n",
    "            length_sph = length_sph.view(length_sph.size(0),1)\n",
    "            A_crat = A_crat.view(A_crat.size(0),1)\n",
    "            A_lrat = A_lrat.view(A_lrat.size(0),1)\n",
    "            Df = Df.view(Df.size(0),1)\n",
    "\n",
    "            #### LOSS\n",
    "            #PHYSICS GUIDED LOSS\n",
    "\n",
    "            loss_pg = 0.0\n",
    "            # regular geom loss, porosity and c_sph known in mascdb\n",
    "            loss_por = criterion(porosity,por_dat)\n",
    "            loss_sar = criterion(sar,sar_dat)\n",
    "            loss_sph = criterion(sph,sph_dat)\n",
    "            loss_c_sph = criterion(cross_sph,c_sph_dat)\n",
    "            loss_l_sph = criterion(length_sph,l_sph_dat)\n",
    "            loss_Acrat = criterion(A_crat,A_crat_dat)\n",
    "            loss_Alrat = criterion(A_lrat,A_lrat_dat)\n",
    "            loss_Df = criterion(Df,Df_dat)\n",
    "            loss_Geom = loss_por + loss_sar + loss_sph + loss_c_sph + loss_l_sph + loss_Acrat + loss_Alrat + loss_Df\n",
    "\n",
    "            test_Geom_loss += loss_Geom * images.size(0)\n",
    "            test_p_Geom_loss += loss_pg * images.size(0)\n",
    "\n",
    "            sph_out.append(sph.item())\n",
    "            c_sph_out.append(cross_sph.item())\n",
    "            l_sph_out.append(length_sph.item())\n",
    "            por_out.append(porosity.item())\n",
    "            sar_out.append(sar.item())\n",
    "            Acrat_out.append(A_crat.item())\n",
    "            Alrat_out.append(A_lrat.item())\n",
    "            Df_out.append(Df.item())\n",
    "\n",
    "            por_ans.append(por_dat.item())\n",
    "            sar_ans.append(sar_dat.item())\n",
    "            sph_ans.append(sph_dat.item())\n",
    "            c_sph_ans.append(c_sph_dat.item())\n",
    "            l_sph_ans.append(l_sph_dat.item())\n",
    "            Acrat_ans.append(A_crat_dat.item())\n",
    "            Alrat_ans.append(A_lrat_dat.item())\n",
    "            Df_ans.append(Df_dat.item())\n",
    "\n",
    "            test_ids.append(id.item())\n",
    "            #err += (abs(outputs - t_Cd)/t_Cd)*100\n",
    "            #print(err)\n",
    "            #loss =  l + loss_mean\n",
    "    #outs_act = np.array(outs)\n",
    "    #answers_act = np.array(answers)\n",
    "    por_out = np.array(por_out)\n",
    "    sar_out = np.array(sar_out)\n",
    "    sph_out = np.array(sph_out)\n",
    "    l_sph_out = np.array(l_sph_out)\n",
    "    c_sph_out = np.array(c_sph_out)\n",
    "    Acrat_out = np.array(Acrat_out)\n",
    "    Alrat_out = np.array(Alrat_out)\n",
    "    Df_out = np.array(Df_out)\n",
    "    por_out = por_out\n",
    "    sar_out = sar_out*0.5\n",
    "    sph_out = sph_out\n",
    "    l_sph_out = l_sph_out*2\n",
    "    c_sph_out = c_sph_out*3.5\n",
    "    Acrat_out = Acrat_out*1.5\n",
    "    Alrat_out = Alrat_out*1.5\n",
    "    Df_out = Df_out*3\n",
    "\n",
    "    por_ans = np.array(por_ans)\n",
    "    sar_ans = np.array(sar_ans)\n",
    "    sph_ans = np.array(sph_ans)\n",
    "    l_sph_ans = np.array(l_sph_ans)\n",
    "    c_sph_ans = np.array(c_sph_ans)\n",
    "    Acrat_ans = np.array(Acrat_ans)\n",
    "    Alrat_ans = np.array(Alrat_ans)\n",
    "    Df_ans = np.array(Df_ans)\n",
    "    por_ans = por_ans\n",
    "    sar_ans = sar_ans*0.5\n",
    "    sph_ans = sph_ans\n",
    "    l_sph_ans = l_sph_ans*2\n",
    "    c_sph_ans = c_sph_ans*3.5\n",
    "    Acrat_ans = Acrat_ans*1.5\n",
    "    Alrat_ans = Alrat_ans*1.5\n",
    "    Df_ans = Df_ans*3\n",
    "\n",
    "    test_ids = np.array(test_ids)\n",
    "por_ans[por_ans==0] = 0.01\n",
    "por_nrmse = np.sqrt(np.sum((por_out - por_ans)**2)/len((por_out - por_ans)**2))/np.mean(por_ans)*100\n",
    "sar_nrmse = np.sqrt(np.sum((sar_out - sar_ans)**2)/len((sar_out - sar_ans)**2))/np.mean(sar_ans)*100\n",
    "sph_nrmse = np.sqrt(np.sum((sph_out - sph_ans)**2)/len((sph_out - sph_ans)**2))/np.mean(sph_ans)*100\n",
    "csph_nrmse = np.sqrt(np.sum((c_sph_out - c_sph_ans)**2)/len((c_sph_out - c_sph_ans)**2))/np.mean(c_sph_ans)*100\n",
    "lsph_nrmse = np.sqrt(np.sum((l_sph_out - l_sph_ans)**2)/len((l_sph_out - l_sph_ans)**2))/np.mean(l_sph_ans)*100\n",
    "Acrat_nrmse = np.sqrt(np.sum((Acrat_out - Acrat_ans)**2)/len((Acrat_out - Acrat_ans)**2))/np.mean(Acrat_ans)*100 \n",
    "Alrat_nrmse = np.sqrt(np.sum((Alrat_out - Alrat_ans)**2)/len((Alrat_out - Alrat_ans)**2))/np.mean(Alrat_ans)*100\n",
    "print('NRMSE scores:')\n",
    "print('Porosity:',por_nrmse,'%')\n",
    "print('Surface area ratio:',sar_nrmse,'%')\n",
    "print('Sphericity:',sph_nrmse,'%')\n",
    "print('Crosswise Sphericity:',csph_nrmse,'%')\n",
    "print('Lengthwise Sphericity:',lsph_nrmse,'%')\n",
    "print('Crosswise area ratio:',Acrat_nrmse,'%')\n",
    "print('Lengthwise area ratio:',Alrat_nrmse,'%')\n",
    "if correlation == 2:\n",
    "    Df_nrmse = np.sqrt(np.sum((Df_out - Df_ans)**2)/len((Df_out - Df_ans)**2))/np.mean(Df_ans)*100\n",
    "    print('Fractal dimension:',Df_nrmse,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
