{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IfsjshmjrdIY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torch import autograd\n",
    "import torch.nn.functional as F\n",
    "import os, os.path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchinfo import summary\n",
    "from torchsummary import summary\n",
    "from torchvision import models\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "import itertools\n",
    "import functools\n",
    "import operator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 1 # LH agg = 1, LH plan = 2, LH gr = 3, LH col = 4, TH gr = 5, KA plan = 6, all data combined = 7\n",
    "correlation = 2 # Holzer and Sommerfeld = 1, Mola = 2, No correlation, pure Cd prediction = 3\n",
    "custom_loss = 1 # 1 = True, 2 = False. Custom loss turned off when correlation = 3\n",
    "pretrained = 1 # 1 = True, 2 = False. Turned off when correlation = 3\n",
    "rstate = 1 #random 5 kfold splits. Any integer; the same integer will always yield the same splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set directory for images and data\n",
    "dir_LH = '/Users/Crazz/Research Codes/LH_dataset.csv'\n",
    "# dir_TH = \n",
    "# dir_KA =\n",
    "dir_LHim = '/Users/Crazz/Research Codes/256res_LH'\n",
    "# dir_KAim =\n",
    "# dir_KAim =\n",
    "\n",
    "## Set directory for desired pre-trained model to load in\n",
    "dir_premodel = '/Users/Crazz/Research Codes/WORKING MODELS/Mola models/aggmola5model.pth'\n",
    "## Set directory and name for saving model. Will overwrite a file of the same name\n",
    "dir_model = '/Users/Crazz/Research Codes/WORKING MODELS/LHagg_HS_model_k' #split num 0-4 and file ext .pth automatically added\n",
    "## Set directory and name for saving prediction results from each split of a model over the whole dataset\n",
    "dir_savdat = '/Users/Crazz/Research Codes/WORKING MODELS/LHagg_HS_outs.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD ALL DATASETS\n",
    "LH_newdf = pd.read_csv(dir_LH)\n",
    "ids = []\n",
    "for i in range(len(LH_newdf)):\n",
    "    picnum = str(LH_newdf['Pic Number'][i])\n",
    "    rollnum = str(LH_newdf['Roll Number'][i])\n",
    "    if len(picnum) == 1:\n",
    "        picnum = '0' + picnum\n",
    "    if len(rollnum) == 1:\n",
    "        rollnum = '0' + rollnum\n",
    "    ids.append(rollnum + '_' + picnum + '.png')\n",
    "LH_newdf['id'] = ids\n",
    "LH_newdf = LH_newdf[LH_newdf['Cd']<13]\n",
    "imgs = {}\n",
    "path = dir_LHim\n",
    "valid_images = [\".jpg\",\".gif\",\".png\",\".tga\"]\n",
    "for f in os.listdir(path):\n",
    "    ext = os.path.splitext(f)[1]\n",
    "    if ext.lower() not in valid_images:\n",
    "        continue\n",
    "    temp = Image.open(os.path.join(path,f))\n",
    "    imgs[f] = temp.copy()\n",
    "    temp.close()\n",
    "\n",
    "img_df = pd.DataFrame(imgs.items(), columns=['id', 'images'])\n",
    "LH_newdf = LH_newdf.merge(img_df, how='inner', on='id')\n",
    "LH_newdf['t_Re'] = np.log(LH_newdf['Re'])\n",
    "LH_newdf['t_Cd'] = np.log(LH_newdf['Cd'])\n",
    "LH_agg = LH_newdf[LH_newdf['aggregate']==1].reset_index(drop=True)\n",
    "LH_agg['tmp_idx'] = range(len(LH_agg))\n",
    "LH_plan = LH_newdf[LH_newdf['planar_crystal']==1].reset_index(drop=True)\n",
    "LH_plan['tmp_idx'] = range(len(LH_plan))\n",
    "LH_gr = LH_newdf[LH_newdf['graupel']==1].reset_index(drop=True)\n",
    "LH_gr['tmp_idx'] = range(len(LH_gr))\n",
    "LH_col = LH_newdf[LH_newdf['columnar_crystal']==1].reset_index(drop=True)\n",
    "LH_col['tmp_idx'] = range(len(LH_col))\n",
    "LH_comb = LH_newdf[LH_newdf['combo']==1].reset_index(drop=True)             #NONE\n",
    "LH_comb['tmp_idx'] = range(len(LH_comb))\n",
    "\n",
    "Theis_df = pd.read_csv(dir_TH)\n",
    "imgs = {}\n",
    "path = dir_THim\n",
    "valid_images = [\".jpg\",\".gif\",\".png\",\".tga\"]\n",
    "for f in os.listdir(path):\n",
    "    ext = os.path.splitext(f)[1]\n",
    "    if ext.lower() not in valid_images:\n",
    "        continue\n",
    "    temp = Image.open(os.path.join(path,f))\n",
    "    imgs[f] = temp.copy()\n",
    "    temp.close()\n",
    "\n",
    "img_df = pd.DataFrame(imgs.items(), columns=['id', 'images'])\n",
    "Theis_df = Theis_df.merge(img_df, how='inner', on='id')\n",
    "Theis_df = Theis_df.rename(columns={'CHA':'area','Dmax':'diam','Mass':'mass','Vel':'vel'})\n",
    "Theis_df['t_Re'] = np.log(Theis_df['Re'])\n",
    "Theis_df['t_Cd'] = np.log(Theis_df['Cd'])\n",
    "Theis_df['tmp_idx'] = np.arange(0,len(Theis_df),1) \n",
    "\n",
    "KA_df = pd.read_csv(dir_KA)\n",
    "imgs = {}\n",
    "path = dir_KAim\n",
    "valid_images = [\".jpg\",\".gif\",\".png\",\".tga\"]\n",
    "for f in os.listdir(path):\n",
    "    ext = os.path.splitext(f)[1]\n",
    "    if ext.lower() not in valid_images:\n",
    "        continue\n",
    "    temp = Image.open(os.path.join(path,f))\n",
    "    imgs[f] = temp.copy()\n",
    "    temp.close()\n",
    "\n",
    "img_df = pd.DataFrame(imgs.items(), columns=['id', 'images'])\n",
    "KA_df = KA_df.merge(img_df, how='inner', on='id')\n",
    "KA_df['t_Re'] = np.log(KA_df['Re'])\n",
    "KA_df['t_Cd'] = np.log(KA_df['Cd'])\n",
    "KA_df['tmp_idx'] = range(len(KA_df))\n",
    "\n",
    "alldat = pd.concat([KA_df,LH_newdf,Theis_df]).reset_index(drop=True)\n",
    "alldat['t_Re'] = np.log(alldat['Re'])\n",
    "alldat['t_Cd'] = np.log(alldat['Cd'])\n",
    "alldat['tmp_idx'] = range(len(alldat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def save_ckp(state, is_best, checkpoint_dir, best_model_dir):\n",
    "    f_path = checkpoint_dir / 'checkpoint.pt'\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath = best_model_dir / 'best_model.pt'\n",
    "        shutil.copyfile(f_path, best_fpath)\n",
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer, checkpoint['epoch']\n",
    "                                                                                                                                                                                                                               \n",
    "\n",
    "class regloader(torch.utils.data.Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X['images']\n",
    "        self.label = X['tmp_idx']\n",
    "        self.A_char = X['area']\n",
    "        self.Re = X['t_Re']\n",
    "        self.Cd = X['t_Cd']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = (self.X[idx])\n",
    "        label = torch.tensor(self.label[idx])\n",
    "        A_char = torch.tensor(self.A_char[idx], dtype=torch.float32)\n",
    "        Re = torch.tensor(self.Re[idx], dtype=torch.float32)\n",
    "        Cd = torch.tensor(self.Cd[idx], dtype=torch.float32)\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomInvert(0.5),\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5), (0.5))]) # might need to change\n",
    "        # seperate ys\n",
    "        return label,transform(X),A_char,Re,Cd\n",
    "    \n",
    "class KGCNN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KGCNN1, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5),#, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 48, 3),# stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(48, 64, 3),# stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 80, 3),# stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.geom = nn.Sequential(\n",
    "            nn.Linear(80 * 14 * 14, 50), # 48*12*12 if size is 256, *4*4 128\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(50, 7), # 7 geoms return\n",
    "            nn.Softplus(),\n",
    "        )\n",
    "    def forward(self, image):\n",
    "        top_out = self.conv(image)\n",
    "        middle = top_out.view(top_out.size(0), -1)\n",
    "        geoms_out = self.geom(middle)\n",
    "        #print(image.size())\n",
    "        #print(middle1.size())\n",
    "        #print(data.size())\n",
    "        return geoms_out\n",
    "    \n",
    "class KGCNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KGCNN2, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5),#, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 48, 3),# stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(48, 64, 3),# stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 80, 3),# stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.geom = nn.Sequential(\n",
    "            nn.Linear(80 * 14 * 14, 50), # 48*12*12 if size is 256, *4*4 128\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(50, 8), # 8 geoms return\n",
    "            nn.Softplus(),\n",
    "        )\n",
    "    def forward(self, image):\n",
    "        top_out = self.conv(image)\n",
    "        middle = top_out.view(top_out.size(0), -1)\n",
    "        geoms_out = self.geom(middle)\n",
    "        #print(image.size())\n",
    "        #print(middle1.size())\n",
    "        #print(data.size())\n",
    "        return geoms_out\n",
    "class KGCNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KGCNN3, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5),#, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 48, 3),# stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(48, 64, 3),# stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 80, 3),# stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.geom = nn.Sequential(\n",
    "            nn.Linear(80 * 14 * 14+1, 50), # 48*12*12 if size is 256, *4*4 128\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(50, 1), # 7 geoms return\n",
    "        )\n",
    "    def forward(self, image,Re):\n",
    "        top_out = self.conv(image)\n",
    "        middle = top_out.view(top_out.size(0), -1)\n",
    "        middle_cd = torch.cat((middle, Re),dim=1)\n",
    "        Cd_out = self.geom(middle_cd)\n",
    "        \n",
    "        #print(image.size())\n",
    "        #print(middle1.size())\n",
    "        #print(data.size())\n",
    "        return Cd_out\n",
    "    \n",
    "# Custom physics loss\n",
    "def physics_loss(sar, sph, length_sph, cross_sph, A_crat, A_lrat):\n",
    "    tiny = 0.000000001\n",
    "    loss_c = (sph/(4*sar) - A_crat*cross_sph)**2\n",
    "    loss_l = (sph/(4*sar) - length_sph*(1/(2*sar) - A_lrat))**2\n",
    "    loss_c = torch.mean(loss_c)\n",
    "    loss_l = torch.mean(loss_l)\n",
    "    # penalize geom outputs if outside range\n",
    "    if torch.max(porosity)>=1:\n",
    "        loss_por = 100*(porosity-0.9)**2\n",
    "    else:\n",
    "        loss_por = torch.tensor(0.000000001, requires_grad=True)\n",
    "    if torch.max(sar) >= 0.5:\n",
    "        loss_sar = 100*(sar-0.4)**2\n",
    "    else:\n",
    "        loss_sar = torch.tensor(0.000000001, requires_grad=True)\n",
    "    if torch.max(sph) >= 1:\n",
    "        loss_sph = 100*(sph-0.9)**2\n",
    "    else:\n",
    "        loss_sph = torch.tensor(0.000000001, requires_grad=True)\n",
    "    loss_pg = torch.mean(loss_c + loss_l + loss_por + loss_sar + loss_sph)\n",
    "    return loss_c, loss_l, loss_pg\n",
    "\n",
    "## Holzer and Sommerfeld correlation\n",
    "def HnS(Re,sph,lsph,csph):\n",
    "    Cd_HS = 8/(Re*torch.sqrt(lsph)) + 16/(Re*torch.sqrt(sph)) + 3/(torch.sqrt(Re)*sph**(3/4)) + (1/csph)*0.4210**(0.4*((-torch.log(sph))**0.2))\n",
    "    return Cd_HS\n",
    "## Mola et al. correlation\n",
    "def Mola(Re,por,sph,lsph,csph,Df):\n",
    "    a1 = 0.10861\n",
    "    a2 = 0.28273\n",
    "    a3 = 0.21479\n",
    "    a4 = 0.65317\n",
    "    a5 = 1.49629\n",
    "    b1 = 8.48137\n",
    "    b2 = 5.07235\n",
    "    b3 = 0.44850\n",
    "    b4 = -0.71\n",
    "    b5 = -1.89037\n",
    "    c1 = 0.66886\n",
    "    c2 = 0.14686\n",
    "    c3 = -0.84224\n",
    "    c4 = 0.02821\n",
    "    c5 = -3.54344\n",
    "    lam1 = 8*(a1*por+a2)*(a3*sph+a4)*Df**a5\n",
    "    lam2 = 16*(b1*por+b2)*(b3*sph+b4)*Df**b5\n",
    "    lam3 = c1*(por**((c2*(1-por)/por)**c3))*(Re**c4)*sph**c5\n",
    "    Cd_M = lam1/(Re*torch.sqrt(csph)) + lam2/(Re*torch.sqrt(sph)) + lam3*(3/(torch.sqrt(Re)*sph**(3/4)) + (1/csph)*0.4210**(0.4*((-torch.log(sph))**0.2)))\n",
    "    return Cd_M\n",
    "\n",
    "if dataset == 1: # LH agg\n",
    "    k_folds = 5\n",
    "    batch_size = 2\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True,random_state=rstate)\n",
    "    lr = 1e-6\n",
    "    num_epochs = 750\n",
    "    fulldata = regloader(LH_agg)\n",
    "    cur_df = LH_agg\n",
    "if dataset == 2: # LH plan\n",
    "    k_folds = 5\n",
    "    batch_size = 2\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True,random_state=rstate)\n",
    "    lr = 1e-6\n",
    "    num_epochs = 750\n",
    "    fulldata = regloader(LH_plan)\n",
    "    cur_df = LH_plan\n",
    "if dataset == 3: # LH graupel\n",
    "    k_folds = 5\n",
    "    batch_size = 6\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True,random_state=rstate)\n",
    "    lr = 1e-5\n",
    "    num_epochs = 750\n",
    "    fulldata = regloader(LH_gr)\n",
    "    cur_df = LH_gr\n",
    "if dataset == 4: # LH col\n",
    "    k_folds = 5\n",
    "    batch_size = 2\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True,random_state=rstate)\n",
    "    lr = 1e-6\n",
    "    num_epochs = 750\n",
    "    fulldata = regloader(LH_col)\n",
    "    cur_df = LH_col\n",
    "if dataset == 5: # TH graupel\n",
    "    k_folds = 5\n",
    "    batch_size = 2\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True,random_state=rstate)\n",
    "    lr = 1e-6\n",
    "    num_epochs = 750\n",
    "    fulldata = regloader(Theis_df)\n",
    "    cur_df = Theis_df\n",
    "if dataset == 6: # KA planar\n",
    "    k_folds = 5\n",
    "    batch_size = 2\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True,random_state=rstate)\n",
    "    lr = 1e-6\n",
    "    num_epochs = 750\n",
    "    fulldata = regloader(KA_df)\n",
    "    cur_df = KA_df\n",
    "if dataset == 7: # all data combined\n",
    "    k_folds = 5\n",
    "    batch_size = 10\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True,random_state=rstate)\n",
    "    lr = 1e-5\n",
    "    num_epochs = 750\n",
    "    fulldata = regloader(alldata)\n",
    "    cur_df = alldata\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if correlation == 1\n",
    "    model = KGCNN1.to(device)\n",
    "if correlation == 2\n",
    "    model = KGCNN2.to(device)\n",
    "if correlation == 3\n",
    "    model = KGCNN3.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss() #MSELoss()  # Mean squared error loss #L1Loss()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "if correlation == 1:\n",
    "    train_Cd_curves = []\n",
    "    train_pg_curves = []\n",
    "\n",
    "    test_Cd_curves = []\n",
    "    test_pg_curves = []\n",
    "\n",
    "    goodplanmodel_epoch = np.array([0,0,0,0,0])\n",
    "\n",
    "    best_test_acc = np.array([100,100,100,100,100])\n",
    "    for fold, (train_ids, test_ids) in enumerate(kf.split(fulldata)):\n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        train_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=batch_size,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(train_ids),\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=batch_size,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(test_ids),\n",
    "        )\n",
    "        model = KGCNN1.to(device)\n",
    "        non_frozen_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.Adam(non_frozen_parameters, lr=lr)  # Choose an optimizer\n",
    "        if pretrained == 1:\n",
    "            model, optimizer, start_epoch = load_ckp(dir_premodel, model, optimizer)\n",
    "\n",
    "        best_test = best_test_acc[fold]\n",
    "        train_llist_Cd = []\n",
    "        train_llist_pg = []\n",
    "\n",
    "        test_llist_Cd = []\n",
    "        test_llist_pg = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()  # Set the model in training mode\n",
    "\n",
    "            train_Cd_loss = 0.0\n",
    "            train_pg_loss = 0.0\n",
    "\n",
    "            for id,images,A_char,Re,Cd in train_loader:\n",
    "\n",
    "                id = id.to(device)\n",
    "                images = Variable(images, requires_grad=True)\n",
    "                A_char = Variable(A_char, requires_grad=True)\n",
    "                Re = Variable(Re, requires_grad=True)\n",
    "                Cd = Variable(Cd, requires_grad=True)\n",
    "\n",
    "                A_char = A_char.view(A_char.size(0),1)\n",
    "                Re = Re.view(Re.size(0),1)\n",
    "                Cd = Cd.view(Cd.size(0),1)\n",
    "\n",
    "                images = images.to(device)\n",
    "                A_char = A_char.to(device)\n",
    "                Re = Re.to(device)\n",
    "                Cd = Cd.to(device)\n",
    "\n",
    "                #with autograd.detect_anomaly():\n",
    "                # Forward pass\n",
    "                geoms_out = model(images)\n",
    "                sph = geoms_out[:,0]\n",
    "                cross_sph = geoms_out[:,1]\n",
    "                length_sph = geoms_out[:,2]\n",
    "                porosity = geoms_out[:,3]\n",
    "                sar = geoms_out[:,4]\n",
    "                A_crat = geoms_out[:,5]\n",
    "                A_lrat = geoms_out[:,6]\n",
    "\n",
    "                sph = sph.view(Re.size(0),1)\n",
    "                cross_sph = cross_sph.view(Re.size(0),1)\n",
    "                length_sph = length_sph.view(Re.size(0),1)\n",
    "                porosity = porosity.view(Re.size(0),1)\n",
    "                sar = sar.view(Re.size(0),1)\n",
    "                A_crat = A_crat.view(A_crat.size(0),1)\n",
    "                A_lrat = A_lrat.view(A_lrat.size(0),1)\n",
    "\n",
    "                loss_c,loss_l,loss_pg = physics_loss(sar*0.5,sph,length_sph*2,cross_sph*3.5,A_crat*1.5,A_lrat*1.5)\n",
    "\n",
    "                Cd_act = HnS(torch.exp(Re),sph,length_sph*2,cross_sph*3.5)#*(torch.abs(1 - porosity))**0.17)*torch.exp(Re)**-0.13)\n",
    "                Cd_logout = torch.log(Cd_act)\n",
    "\n",
    "                # TOTAL loss w/o phys Cd\n",
    "                # sum physics guided and regular losses from Cd and geom parameters\n",
    "                if torch.max(sph) >= 1:\n",
    "                    loss_Cd = 2*torch.abs(torch.max(sph)-0.5)**2\n",
    "                else:\n",
    "                    loss_Cd = criterion(Cd_logout,Cd)\n",
    "                loss_Cd = loss_Cd.to(device)\n",
    "                loss_tot = loss_Cd + loss_pg\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                if custom_loss == 1:\n",
    "                    loss_tot.backward()\n",
    "                if custom_loss == 2:\n",
    "                    loss_Cd.backward()\n",
    "                #nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                clip_value = 10\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                #####\n",
    "                train_Cd_loss += loss_Cd * id.size(0)\n",
    "                train_pg_loss += loss_pg * id.size(0)\n",
    "\n",
    "            train_Cd_loss /= len(train_loader.dataset)\n",
    "            train_pg_loss /= len(train_loader.dataset)\n",
    "\n",
    "            train_llist_pg.append(train_pg_loss.item())\n",
    "\n",
    "            # Evaluation\n",
    "            model.eval()  # Set the model in evaluation mode\n",
    "\n",
    "            test_Cd_loss = 0.0\n",
    "            test_pg_loss = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for id,images,A_char,Re,Cd in test_loader:\n",
    "\n",
    "                    id = id.to(device)\n",
    "                    images = Variable(images, requires_grad=True)\n",
    "                    A_char = Variable(A_char, requires_grad=True)\n",
    "                    Re = Variable(Re, requires_grad=True)\n",
    "                    Cd = Variable(Cd, requires_grad=True)\n",
    "\n",
    "                    A_char = A_char.view(A_char.size(0),1)\n",
    "                    Re = Re.view(Re.size(0),1)\n",
    "                    Cd = Cd.view(Cd.size(0),1)\n",
    "\n",
    "                    images = images.to(device)\n",
    "                    A_char = A_char.to(device)\n",
    "                    Re = Re.to(device)\n",
    "                    Cd = Cd.to(device)\n",
    "\n",
    "                    #with autograd.detect_anomaly():\n",
    "                    # Forward pass\n",
    "                    geoms_out = model(images)\n",
    "                    sph = geoms_out[:,0]\n",
    "                    cross_sph = geoms_out[:,1]\n",
    "                    length_sph = geoms_out[:,2]\n",
    "                    porosity = geoms_out[:,3]\n",
    "                    sar = geoms_out[:,4]\n",
    "                    A_crat = geoms_out[:,5]\n",
    "                    A_lrat = geoms_out[:,6]\n",
    "\n",
    "                    sph = sph.view(Re.size(0),1)\n",
    "                    cross_sph = cross_sph.view(Re.size(0),1)\n",
    "                    length_sph = length_sph.view(Re.size(0),1)\n",
    "                    porosity = porosity.view(Re.size(0),1)\n",
    "                    sar = sar.view(Re.size(0),1)\n",
    "                    A_crat = A_crat.view(A_crat.size(0),1)\n",
    "                    A_lrat = A_lrat.view(A_lrat.size(0),1)\n",
    "\n",
    "                    loss_c,loss_l,loss_pg = physics_loss(sar*0.5,sph,length_sph*2,cross_sph*3.5,A_crat*1.5,A_lrat*1.5)\n",
    "\n",
    "                    Cd_act = HnS(torch.exp(Re),sph,length_sph*2,cross_sph*3.5)#*(torch.abs(1 - porosity))**0.17)*torch.exp(Re)**-0.13)\n",
    "                    Cd_logout = torch.log(Cd_act)\n",
    "\n",
    "                    # TOTAL loss w/o phys Cd\n",
    "                    # sum physics guided and regular losses from Cd and geom parameters\n",
    "                    loss_Cd = criterion(Cd_logout,Cd)\n",
    "\n",
    "                    loss_tot = loss_Cd + loss_pg\n",
    "                    # TOTAL loss w/o phys Cd\n",
    "                    # sum physics guided and regular losses from Cd and geom parameters\n",
    "                    test_Cd_loss += loss_Cd * id.size(0)\n",
    "                    test_pg_loss += loss_pg * id.size(0)\n",
    "\n",
    "            test_Cd_loss /= len(test_loader.dataset)\n",
    "            test_pg_loss /= len(test_loader.dataset)\n",
    "\n",
    "            test_llist_Cd.append(test_Cd_loss.item())\n",
    "            test_llist_pg.append(test_pg_loss.item())\n",
    "\n",
    "            if test_Cd_loss < best_test:\n",
    "                torch.save(model.state_dict(), dir_model+str(fold)+'.pth')\n",
    "                goodplanmodel_epoch[fold] = epoch\n",
    "                best_test = test_Cd_loss\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Train Cd Loss: {train_Cd_loss:.6f} - Test Loss: {test_Cd_loss:.6f}\")\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Train pg Loss: {train_pg_loss:.6f} - Test Loss: {test_pg_loss:.6f}\")\n",
    "            print('\\n')\n",
    "            #print(Cd_out)\n",
    "        train_Cd_curves.append(train_llist_Cd)\n",
    "        test_Cd_curves.append(test_llist_Cd)\n",
    "        train_pg_curves.append(train_llist_pg)\n",
    "        test_pg_curves.append(test_llist_pg)\n",
    "\n",
    "if correlation == 2:\n",
    "    train_Cd_curves = []\n",
    "    train_pg_curves = []\n",
    "\n",
    "    test_Cd_curves = []\n",
    "    test_pg_curves = []\n",
    "\n",
    "    best_test_acc = np.array([100,100,100,100,100])\n",
    "\n",
    "    # Training loop\n",
    "\n",
    "    for fold, (train_ids, test_ids) in enumerate(kf.split(fulldata)):\n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        train_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=batch_size,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(train_ids),\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=batch_size,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(test_ids),\n",
    "        )\n",
    "        model = KGCNN2.to(device)\n",
    "        non_frozen_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.Adam(non_frozen_parameters, lr=lr)  # Choose an optimizer\n",
    "        \n",
    "        if pretrained == 1:\n",
    "            model, optimizer, start_epoch = load_ckp(dir_premodel, model, optimizer)\n",
    "    \n",
    "        best_test = best_test_acc[fold]\n",
    "\n",
    "        train_llist_Cd = []\n",
    "        train_llist_pg = []\n",
    "        test_llist_Cd = []\n",
    "        test_llist_pg = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()  # Set the model in training mode\n",
    "\n",
    "            train_Cd_loss = 0.0\n",
    "            train_Df_loss = 0.0\n",
    "            train_pg_loss = 0.0\n",
    "\n",
    "            for id,images,A_char,Re,Cd,Df_dat in train_loader:\n",
    "\n",
    "                id = id.to(device)\n",
    "                images = Variable(images, requires_grad=True)\n",
    "                A_char = Variable(A_char, requires_grad=True)\n",
    "                Re = Variable(Re, requires_grad=True)\n",
    "                Cd = Variable(Cd, requires_grad=True)\n",
    "                Df_dat = Variable(Df_dat, requires_grad=True)\n",
    "\n",
    "                A_char = A_char.view(A_char.size(0),1)\n",
    "                Re = Re.view(Re.size(0),1)\n",
    "                Cd = Cd.view(Cd.size(0),1)\n",
    "                Df_dat = Df_dat.view(Df_dat.size(0),1)\n",
    "\n",
    "                images = images.to(device)\n",
    "                A_char = A_char.to(device)\n",
    "                Re = Re.to(device)\n",
    "                Cd = Cd.to(device)\n",
    "                Df_dat = Df_dat.to(device)\n",
    "\n",
    "                #with autograd.detect_anomaly():\n",
    "                # Forward pass\n",
    "                geoms_out = model(images)\n",
    "                sph = geoms_out[:,0]\n",
    "                cross_sph = geoms_out[:,1]\n",
    "                length_sph = geoms_out[:,2]\n",
    "                porosity = geoms_out[:,3]\n",
    "                sar = geoms_out[:,4]\n",
    "                A_crat = geoms_out[:,5]\n",
    "                A_lrat = geoms_out[:,6]\n",
    "                Df = geoms_out[:,7]\n",
    "\n",
    "                sph = sph.view(Re.size(0),1)\n",
    "                cross_sph = cross_sph.view(Re.size(0),1)\n",
    "                length_sph = length_sph.view(Re.size(0),1)\n",
    "                porosity = porosity.view(Re.size(0),1)\n",
    "                sar = sar.view(Re.size(0),1)\n",
    "                A_crat = A_crat.view(A_crat.size(0),1)\n",
    "                A_lrat = A_lrat.view(A_lrat.size(0),1)\n",
    "                Df = Df.view(Df.size(0),1)\n",
    "\n",
    "                loss_c,loss_l,loss_pg = physics_loss(sar*0.5,sph,length_sph*2,cross_sph*3.5,A_crat*1.5,A_lrat*1.5)\n",
    "\n",
    "                Cd_act = Mola(torch.exp(Re),porosity,sph,length_sph*2,cross_sph*3.5,Df*3*1.3)\n",
    "                Cd_logout = torch.log(Cd_act)\n",
    "\n",
    "                # TOTAL loss w/o phys Cd\n",
    "                # sum physics guided and regular losses from Cd and geom parameters\n",
    "                if torch.max(sph) >= 1 or torch.max(porosity) >= 1:\n",
    "                    loss_Cd = torch.tensor(0.2,requires_grad=True)\n",
    "                elif torch.min(Cd_act) <= 0.0:\n",
    "                    loss_Cd = torch.abs(torch.min(Cd_act)*50)\n",
    "                else:\n",
    "                    loss_Cd = criterion(Cd_logout,Cd)\n",
    "\n",
    "                if torch.max(Df*3*1.3) >= 3:\n",
    "                    loss_Df = torch.abs(torch.max(Df*3*1.3)-2.9)**2\n",
    "                elif torch.min(Df*3*1.3) <= 1.4:\n",
    "                    loss_Df = torch.abs(torch.min(Df*3*1.3)-1.5)**2\n",
    "                else:\n",
    "                    loss_Df = criterion(Df*3*1.3,Df_dat*3*1.3)/8\n",
    "\n",
    "                loss_Df = loss_Df.to(device)\n",
    "                loss_Cd = loss_Cd.to(device)\n",
    "                loss_tot = loss_Cd + loss_Df + loss_pg\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                if custom_loss == 1:\n",
    "                    loss_tot.backward()\n",
    "                if custom_loss == 2:\n",
    "                    (loss_Cd+loss_Df).backward()\n",
    "                \n",
    "                #nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                clip_value = 10\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                #####\n",
    "                train_Cd_loss += loss_Cd * id.size(0)\n",
    "                train_Df_loss += loss_Df* id.size(0)\n",
    "                train_pg_loss += loss_pg * id.size(0)\n",
    "\n",
    "            train_Cd_loss /= len(train_loader.dataset)\n",
    "            train_Df_loss /= len(train_loader.dataset)\n",
    "            train_pg_loss /= len(train_loader.dataset)\n",
    "\n",
    "            train_llist_pg.append(train_pg_loss.item())\n",
    "\n",
    "            # Evaluation\n",
    "            model.eval()  # Set the model in evaluation mode\n",
    "\n",
    "            test_Cd_loss = 0.0\n",
    "            test_Df_loss = 0.0\n",
    "            test_pg_loss = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for id,images,A_char,Re,Cd,Df_dat in test_loader:\n",
    "\n",
    "                    id = id.to(device)\n",
    "                    images = Variable(images, requires_grad=True)\n",
    "                    A_char = Variable(A_char, requires_grad=True)\n",
    "                    Re = Variable(Re, requires_grad=True)\n",
    "                    Cd = Variable(Cd, requires_grad=True)\n",
    "                    Df_dat = Variable(Df_dat, requires_grad=True)\n",
    "\n",
    "                    A_char = A_char.view(A_char.size(0),1)\n",
    "                    Re = Re.view(Re.size(0),1)\n",
    "                    Cd = Cd.view(Cd.size(0),1)\n",
    "                    Df_dat = Df_dat.view(Df_dat.size(0),1)\n",
    "\n",
    "                    images = images.to(device)\n",
    "                    A_char = A_char.to(device)\n",
    "                    Re = Re.to(device)\n",
    "                    Cd = Cd.to(device)\n",
    "                    Df_dat = Df_dat.to(device)\n",
    "\n",
    "                    #with autograd.detect_anomaly():\n",
    "                    # Forward pass\n",
    "                    geoms_out = model(images)\n",
    "                    sph = geoms_out[:,0]\n",
    "                    cross_sph = geoms_out[:,1]\n",
    "                    length_sph = geoms_out[:,2]\n",
    "                    porosity = geoms_out[:,3]\n",
    "                    sar = geoms_out[:,4]\n",
    "                    A_crat = geoms_out[:,5]\n",
    "                    A_lrat = geoms_out[:,6]\n",
    "                    Df = geoms_out[:,7]\n",
    "\n",
    "                    sph = sph.view(Re.size(0),1)\n",
    "                    cross_sph = cross_sph.view(Re.size(0),1)\n",
    "                    length_sph = length_sph.view(Re.size(0),1)\n",
    "                    porosity = porosity.view(Re.size(0),1)\n",
    "                    sar = sar.view(Re.size(0),1)\n",
    "                    A_crat = A_crat.view(A_crat.size(0),1)\n",
    "                    A_lrat = A_lrat.view(A_lrat.size(0),1)\n",
    "                    Df = Df.view(Df.size(0),1)\n",
    "\n",
    "                    loss_c,loss_l,loss_pg = physics_loss(sar*0.5,sph,length_sph*2,cross_sph*3.5,A_crat*1.5,A_lrat*1.5)\n",
    "\n",
    "                    Cd_act = Mola(torch.exp(Re),porosity,sph,length_sph*2,cross_sph*3.5,Df*3*1.3)\n",
    "                    Cd_logout = torch.log(Cd_act)\n",
    "\n",
    "                    # TOTAL loss w/o phys Cd\n",
    "                    # sum physics guided and regular losses from Cd and geom parameters\n",
    "                    loss_Cd = criterion(Cd_logout,Cd)\n",
    "                    loss_Df = criterion(Df*3*1.3,Df_dat*3*1.3)/8\n",
    "                    loss_tot = loss_Cd + loss_Df + loss_pg\n",
    "                    # TOTAL loss w/o phys Cd\n",
    "                    # sum physics guided and regular losses from Cd and geom parameters\n",
    "                    test_Cd_loss += loss_Cd * id.size(0)\n",
    "                    test_Df_loss += loss_Df * id.size(0)\n",
    "                    test_pg_loss += loss_pg * id.size(0)\n",
    "\n",
    "            test_Cd_loss /= len(test_loader.dataset)\n",
    "            test_Df_loss /= len(test_loader.dataset)\n",
    "            test_pg_loss /= len(test_loader.dataset)\n",
    "\n",
    "            test_llist_Cd.append(test_Cd_loss.item())\n",
    "            test_llist_pg.append(test_pg_loss.item())\n",
    "\n",
    "            if test_Cd_loss < best_test:\n",
    "                torch.save(model.state_dict(), dir_model+str(fold)+'.pth')\n",
    "                goodplanmodel_epoch = epoch\n",
    "                best_test = test_Cd_loss\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Train Cd Loss: {train_Cd_loss:.6f} - Test Loss: {test_Cd_loss:.6f}\")\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Train Df Loss: {train_Df_loss:.6f} - Test Loss: {test_Df_loss:.6f}\")\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Train pg Loss: {train_pg_loss:.6f} - Test Loss: {test_pg_loss:.6f}\")\n",
    "            print('\\n')\n",
    "            #print(Cd_out)\n",
    "        train_Cd_curves.append(train_llist_Cd)\n",
    "        test_Cd_curves.append(test_llist_Cd)\n",
    "        train_pg_curves.append(train_llist_pg)\n",
    "        test_pg_curves.append(test_llist_pg)\n",
    "\n",
    "if correlation == 3:\n",
    "    train_llist_Cd = []\n",
    "    test_llist_Cd = []\n",
    "    goodplanmodel_epoch = np.array([0,0,0,0,0])\n",
    "\n",
    "    best_test_acc = np.array([100,100,100,100,100])\n",
    "\n",
    "    # Training loop\n",
    "\n",
    "    for fold, (train_ids, test_ids) in enumerate(kf.split(fulldata)):\n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        train_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=batch_size,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(train_ids),\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=batch_size,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(test_ids),\n",
    "        )\n",
    "        model = KGCNN3.to(device)\n",
    "        non_frozen_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.Adam(non_frozen_parameters, lr=lr)  # Choose an optimizer\n",
    "        best_test = best_test_acc[fold]\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()  # Set the model in training mode\n",
    "\n",
    "            train_Cd_loss = 0.0\n",
    "\n",
    "            for id,images,A_char,Re,Cd in train_loader:\n",
    "\n",
    "                id = id.to(device)\n",
    "                images = Variable(images, requires_grad=True)\n",
    "                A_char = Variable(A_char, requires_grad=True)\n",
    "                Re = Variable(Re, requires_grad=True)\n",
    "                Cd = Variable(Cd, requires_grad=True)\n",
    "\n",
    "                A_char = A_char.view(A_char.size(0),1)\n",
    "                Re = Re.view(Re.size(0),1)\n",
    "                Cd = Cd.view(Cd.size(0),1)\n",
    "\n",
    "                images = images.to(device)\n",
    "                A_char = A_char.to(device)\n",
    "                Re = Re.to(device)\n",
    "                Cd = Cd.to(device)\n",
    "\n",
    "                #with autograd.detect_anomaly():\n",
    "                # Forward pass\n",
    "                cd_out = model(images,Re)\n",
    "\n",
    "                loss_Cd = criterion(cd_out,Cd)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss_Cd.backward()\n",
    "                #nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                clip_value = 10\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                #####\n",
    "                train_Cd_loss += loss_Cd * id.size(0)\n",
    "\n",
    "            train_Cd_loss /= len(train_loader.dataset)\n",
    "\n",
    "            # Evaluation\n",
    "            model.eval()  # Set the model in evaluation mode\n",
    "\n",
    "            test_Cd_loss = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for id,images,A_char,Re,Cd in test_loader:\n",
    "\n",
    "                    id = id.to(device)\n",
    "                    images = Variable(images, requires_grad=True)\n",
    "                    A_char = Variable(A_char, requires_grad=True)\n",
    "                    Re = Variable(Re, requires_grad=True)\n",
    "                    Cd = Variable(Cd, requires_grad=True)\n",
    "\n",
    "                    A_char = A_char.view(A_char.size(0),1)\n",
    "                    Re = Re.view(Re.size(0),1)\n",
    "                    Cd = Cd.view(Cd.size(0),1)\n",
    "\n",
    "                    images = images.to(device)\n",
    "                    A_char = A_char.to(device)\n",
    "                    Re = Re.to(device)\n",
    "                    Cd = Cd.to(device)\n",
    "\n",
    "                    #with autograd.detect_anomaly():\n",
    "                    # Forward pass\n",
    "                    cd_out = model(images,Re)\n",
    "\n",
    "                    loss_Cd = criterion(cd_out,Cd)\n",
    "\n",
    "                    # TOTAL loss w/o phys Cd\n",
    "                    # sum physics guided and regular losses from Cd and geom parameters\n",
    "                    test_Cd_loss += loss_Cd * id.size(0)\n",
    "\n",
    "            test_Cd_loss /= len(test_loader.dataset)\n",
    "\n",
    "            test_llist_Cd.append(test_Cd_loss.item())\n",
    "\n",
    "            if test_Cd_loss < best_test:\n",
    "                torch.save(model.state_dict(), dir_model+str(fold)+'.pth')\n",
    "                goodplanmodel_epoch[fold] = epoch\n",
    "                best_test = test_Cd_loss\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Train Cd Loss: {train_Cd_loss:.6f} - Test Loss: {test_Cd_loss:.6f}\")\n",
    "            print('\\n')\n",
    "print('Completed training.')\n",
    "print('Now evaluating performance')\n",
    "\n",
    "if correlation == 1:\n",
    "    hs_planout = pd.DataFrame(columns=['por','sar','sph','lsph','csph','Acrat','Alrat','Cdout','massout','Rhos','Rhoc','Rhol','id','Cd','Re','mass'])\n",
    "    x,y,z,w,v = kf.split(fulldata)\n",
    "    model = KGCNN1.to(device)\n",
    "    for i in range(5):\n",
    "        model.load_state_dict(torch.load(dir_model+str(i)+\".pth\"))\n",
    "        if i == 0:\n",
    "            test_loader = DataLoader(\n",
    "                dataset=fulldata,\n",
    "                batch_size=1,\n",
    "                sampler=torch.utils.data.SubsetRandomSampler(x[1])\n",
    "            )\n",
    "        elif i == 1:\n",
    "            test_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=1,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(y[1])\n",
    "            )\n",
    "        elif i == 2:\n",
    "            test_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=1,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(z[1])\n",
    "            )\n",
    "        elif i == 3:\n",
    "            test_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=1,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(w[1])\n",
    "            )\n",
    "        elif i == 4:\n",
    "            test_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=1,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(v[1])\n",
    "            )\n",
    "        model.eval()  # Set the model in evaluation mode\n",
    "\n",
    "        Cd_outs = []\n",
    "        por_outs = []\n",
    "        sar_outs = []\n",
    "        sph_outs = []\n",
    "        csph_outs = []\n",
    "        lsph_outs = []\n",
    "        Acrat_outs = []\n",
    "        Alrat_outs = []\n",
    "\n",
    "        Cd_ans = []\n",
    "\n",
    "        test_ids = []\n",
    "        with torch.no_grad():\n",
    "            for id,images,A_char,Re,Cd in test_loader:\n",
    "\n",
    "                id = id.to(device)\n",
    "                images = Variable(images, requires_grad=True)\n",
    "                A_char = Variable(A_char, requires_grad=True)\n",
    "                Re = Variable(Re, requires_grad=True)\n",
    "                Cd = Variable(Cd, requires_grad=True)\n",
    "\n",
    "                A_char = A_char.view(A_char.size(0),1)\n",
    "                Re = Re.view(Re.size(0),1)\n",
    "                Cd = Cd.view(Cd.size(0),1)\n",
    "\n",
    "                images = images.to(device)\n",
    "                A_char = A_char.to(device)\n",
    "                Re = Re.to(device)\n",
    "                Cd = Cd.to(device)\n",
    "\n",
    "                #with autograd.detect_anomaly():\n",
    "                # Forward pass\n",
    "                geoms_out = model(images)\n",
    "                sph = geoms_out[:,0]\n",
    "                cross_sph = geoms_out[:,1]\n",
    "                length_sph = geoms_out[:,2]\n",
    "                porosity = geoms_out[:,3]\n",
    "                sar = geoms_out[:,4]\n",
    "                A_crat = geoms_out[:,5]\n",
    "                A_lrat = geoms_out[:,6]\n",
    "\n",
    "                sph = sph.view(Re.size(0),1)\n",
    "                cross_sph = cross_sph.view(Re.size(0),1)\n",
    "                length_sph = length_sph.view(Re.size(0),1)\n",
    "                porosity = porosity.view(Re.size(0),1)\n",
    "                sar = sar.view(Re.size(0),1)\n",
    "                A_crat = A_crat.view(A_crat.size(0),1)\n",
    "                A_lrat = A_lrat.view(A_lrat.size(0),1)\n",
    "\n",
    "                Cd_out = HnS(torch.exp(Re),sph,length_sph*2,cross_sph*3.5)#*(torch.abs(1 - porosity))**0.17)*torch.exp(Re)**-0.13)\n",
    "                Cd_logout = torch.log(Cd_out)\n",
    "\n",
    "                Cd_outs.append(Cd_out.item())\n",
    "                por_outs.append(porosity.item())\n",
    "                sar_outs.append(sar.item())\n",
    "                sph_outs.append(sph.item())\n",
    "                lsph_outs.append(length_sph.item())\n",
    "                csph_outs.append(cross_sph.item())\n",
    "                Acrat_outs.append(A_crat.item())\n",
    "                Alrat_outs.append(A_lrat.item())\n",
    "\n",
    "                Cd_ans.append(torch.exp(Cd).item())\n",
    "\n",
    "                test_ids.append(id.item())\n",
    "                #err += (abs(outputs - t_Cd)/t_Cd)*100\n",
    "                #print(err)\n",
    "                #loss =  l + loss_mean\n",
    "        #outs_act = np.array(outs)\n",
    "        #answers_act = np.array(answers)\n",
    "        Cd_outs = np.array(Cd_outs)\n",
    "        Cd_acts = np.array(Cd_outs)\n",
    "\n",
    "        por_outs = np.array(por_outs)\n",
    "        sar_outs = np.array(sar_outs)\n",
    "        sph_outs = np.array(sph_outs)\n",
    "        lsph_outs = np.array(lsph_outs)\n",
    "        csph_outs = np.array(csph_outs)\n",
    "        Acrat_outs = np.array(Acrat_outs)\n",
    "        Alrat_outs = np.array(Alrat_outs)\n",
    "        por_outs = por_outs\n",
    "        sar_outs = sar_outs*0.5\n",
    "        sph_outs = sph_outs\n",
    "        l_sph_outs = lsph_outs*2\n",
    "        c_sph_outs = csph_outs*3.5\n",
    "        Acrat_outs = Acrat_outs*1.5\n",
    "        Alrat_outs = Alrat_outs*1.5\n",
    "\n",
    "        Cd_ans = np.array(Cd_ans)\n",
    "        Cd_actans = np.array(Cd_ans)\n",
    "\n",
    "        test_ids = np.array(test_ids)\n",
    "        mass_out = Cd_acts*0.5*cur_df['rho'][test_ids]*(cur_df['vel'][test_ids]**2)*cur_df['area'][test_ids]/9.81\n",
    "        Cd_errs = ((Cd_ans - Cd_outs)/abs(Cd_ans))*100\n",
    "        Cd_errs_abs = (abs(Cd_ans - Cd_outs)/abs(Cd_ans))*100\n",
    "        Cd_aerrs = -((Cd_actans - Cd_acts)/abs(Cd_actans))*100\n",
    "        Cd_aerrs_abs = (abs(Cd_actans - Cd_acts)/abs(Cd_actans))*100\n",
    "        Cd_mse = (Cd_acts - Cd_actans)**2\n",
    "        Cd_rmse = np.sqrt(np.sum(Cd_mse)/len(Cd_mse))\n",
    "        Cd_nrmse = (Cd_rmse/np.mean(Cd_actans))*100\n",
    "        mass_mse = (mass_out - cur_df['mass'][test_ids])**2\n",
    "        mass_rmse = np.sqrt(np.sum(mass_mse)/len(mass_mse))\n",
    "        Vol_out_s = 4*np.pi/3*(sph_outs/(4*sar_outs) * cur_df['area'][test_ids]/np.pi)**(3/2)\n",
    "        rho_s = mass_out/Vol_out_s\n",
    "        Vol_out_c = 4*np.pi/3*(c_sph_outs*Acrat_outs * cur_df['area'][test_ids]/np.pi)**(3/2)\n",
    "        rho_c = mass_out/Vol_out_c\n",
    "        Vol_out_l = 4*np.pi/3*(l_sph_outs*(1/(2*sar_outs)-Alrat_outs) * cur_df['area'][test_ids]/np.pi)**(3/2)\n",
    "        rho_l = mass_out/Vol_out_l\n",
    "        predout = pd.DataFrame(por_outs,columns=['por'])\n",
    "        predout['sar'] = sar_outs\n",
    "        predout['sph'] = sph_outs\n",
    "        predout['lsph'] = l_sph_outs\n",
    "        predout['csph'] = c_sph_outs\n",
    "        predout['Acrat'] = Acrat_outs\n",
    "        predout['Alrat'] = Alrat_outs\n",
    "        predout['Cdout'] = Cd_outs\n",
    "        predout['massout'] = np.array(mass_out)\n",
    "        predout['Rhos'] = np.array(rho_s)\n",
    "        predout['Rhoc'] = np.array(rho_c)\n",
    "        predout['Rhol'] = np.array(rho_l)\n",
    "        predout['id'] = np.array(cur_df['id'][test_ids])\n",
    "        predout['Cd'] = np.array(cur_df['Cd'][test_ids])\n",
    "        predout['Re'] = np.array(cur_df['Re'][test_ids])\n",
    "        predout['mass'] = np.array(cur_df['mass'][test_ids])\n",
    "        hs_planout = pd.concat([hs_planout,predout],ignore_index=True)\n",
    "        \n",
    "if correlation == 2:\n",
    "    hs_planout = pd.DataFrame(columns=['por','sar','sph','lsph','csph','Acrat','Alrat','Df_out','Cdout','massout','Rhos','Rhoc','Rhol','id','Cd','Re','Df','mass'])\n",
    "    x,y,z,w,v = kf.split(fulldata)\n",
    "    model = KGCNN2.to(device)\n",
    "    for i in range(5):\n",
    "        model.load_state_dict(torch.load(dir_model+str(i)+\".pth\"))\n",
    "        if i == 0:\n",
    "            test_loader = DataLoader(\n",
    "                dataset=fulldata,\n",
    "                batch_size=1,\n",
    "                sampler=torch.utils.data.SubsetRandomSampler(x[1])\n",
    "            )\n",
    "        elif i == 1:\n",
    "            test_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=1,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(y[1])\n",
    "            )\n",
    "        elif i == 2:\n",
    "            test_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=1,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(z[1])\n",
    "            )\n",
    "        elif i == 3:\n",
    "            test_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=1,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(w[1])\n",
    "            )\n",
    "        elif i == 4:\n",
    "            test_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=1,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(v[1])\n",
    "            )\n",
    "        model.eval()  # Set the model in evaluation mode\n",
    "\n",
    "        Cd_outs = []\n",
    "        por_outs = []\n",
    "        sar_outs = []\n",
    "        sph_outs = []\n",
    "        csph_outs = []\n",
    "        lsph_outs = []\n",
    "        Acrat_outs = []\n",
    "        Alrat_outs = []\n",
    "        Df_outs = []\n",
    "\n",
    "        Cd_ans = []\n",
    "        Df_ans = []\n",
    "\n",
    "        test_ids = []\n",
    "        with torch.no_grad():\n",
    "            for id,images,A_char,Re,Cd,Df_dat in test_loader:\n",
    "\n",
    "                id = id.to(device)\n",
    "                images = Variable(images, requires_grad=True)\n",
    "                A_char = Variable(A_char, requires_grad=True)\n",
    "                Re = Variable(Re, requires_grad=True)\n",
    "                Cd = Variable(Cd, requires_grad=True)\n",
    "                Df_dat = Variable(Df_dat, requires_grad=True)\n",
    "\n",
    "                A_char = A_char.view(A_char.size(0),1)\n",
    "                Re = Re.view(Re.size(0),1)\n",
    "                Cd = Cd.view(Cd.size(0),1)\n",
    "                Df_dat = Df_dat.view(Df_dat.size(0),1)\n",
    "\n",
    "                images = images.to(device)\n",
    "                A_char = A_char.to(device)\n",
    "                Re = Re.to(device)\n",
    "                Cd = Cd.to(device)\n",
    "                Df_dat = Df_dat.to(device)\n",
    "\n",
    "                #with autograd.detect_anomaly():\n",
    "                # Forward pass\n",
    "                geoms_out = model(images)\n",
    "                sph = geoms_out[:,0]\n",
    "                cross_sph = geoms_out[:,1]\n",
    "                length_sph = geoms_out[:,2]\n",
    "                porosity = geoms_out[:,3]\n",
    "                sar = geoms_out[:,4]\n",
    "                A_crat = geoms_out[:,5]\n",
    "                A_lrat = geoms_out[:,6]\n",
    "                Df = geoms_out[:,7]\n",
    "\n",
    "                sph = sph.view(Re.size(0),1)\n",
    "                cross_sph = cross_sph.view(Re.size(0),1)\n",
    "                length_sph = length_sph.view(Re.size(0),1)\n",
    "                porosity = porosity.view(Re.size(0),1)\n",
    "                sar = sar.view(Re.size(0),1)\n",
    "                A_crat = A_crat.view(A_crat.size(0),1)\n",
    "                A_lrat = A_lrat.view(A_lrat.size(0),1)\n",
    "                Df = Df.view(Df.size(0),1)\n",
    "\n",
    "                Cd_out = Mola(torch.exp(Re),porosity,sph,length_sph*2,cross_sph*3.5,Df*3*1.3)\n",
    "                Cd_logout = torch.log(Cd_out)\n",
    "\n",
    "                Cd_outs.append(Cd_out.item())\n",
    "                por_outs.append(porosity.item())\n",
    "                sar_outs.append(sar.item())\n",
    "                sph_outs.append(sph.item())\n",
    "                lsph_outs.append(length_sph.item())\n",
    "                csph_outs.append(cross_sph.item())\n",
    "                Acrat_outs.append(A_crat.item())\n",
    "                Alrat_outs.append(A_lrat.item())\n",
    "                Df_outs.append(Df.item())\n",
    "\n",
    "                Cd_ans.append(torch.exp(Cd).item())\n",
    "                Df_ans.append(Df_dat.item())\n",
    "\n",
    "                test_ids.append(id.item())\n",
    "                #err += (abs(outputs - t_Cd)/t_Cd)*100\n",
    "                #print(err)\n",
    "                #loss =  l + loss_mean\n",
    "        #outs_act = np.array(outs)\n",
    "        #answers_act = np.array(answers)\n",
    "        Cd_outs = np.array(Cd_outs)\n",
    "        Cd_acts = np.array(Cd_outs)\n",
    "\n",
    "        por_outs = np.array(por_outs)\n",
    "        sar_outs = np.array(sar_outs)\n",
    "        sph_outs = np.array(sph_outs)\n",
    "        lsph_outs = np.array(lsph_outs)\n",
    "        csph_outs = np.array(csph_outs)\n",
    "        Acrat_outs = np.array(Acrat_outs)\n",
    "        Alrat_outs = np.array(Alrat_outs)\n",
    "        Df_outs = np.array(Df_outs)\n",
    "        por_outs = por_outs\n",
    "        sar_outs = sar_outs*0.5\n",
    "        sph_outs = sph_outs\n",
    "        l_sph_outs = lsph_outs*2\n",
    "        c_sph_outs = csph_outs*3.5\n",
    "        Acrat_outs = Acrat_outs*1.5\n",
    "        Alrat_outs = Alrat_outs*1.5\n",
    "        Df_outs = Df_outs*3*1.3\n",
    "\n",
    "        Cd_ans = np.array(Cd_ans)\n",
    "        Cd_actans = np.array(Cd_ans)\n",
    "        Df_ans = np.array(Df_ans)*3*1.3\n",
    "\n",
    "        test_ids = np.array(test_ids)\n",
    "        mass_out = Cd_acts*0.5*cur_df['rho'][test_ids]*(cur_df['vel'][test_ids]**2)*cur_df['area'][test_ids]/9.81\n",
    "        Cd_errs = ((Cd_ans - Cd_outs)/abs(Cd_ans))*100\n",
    "        Cd_errs_abs = (abs(Cd_ans - Cd_outs)/abs(Cd_ans))*100\n",
    "        Cd_aerrs = -((Cd_actans - Cd_acts)/abs(Cd_actans))*100\n",
    "        Cd_aerrs_abs = (abs(Cd_actans - Cd_acts)/abs(Cd_actans))*100\n",
    "        Cd_mse = (Cd_acts - Cd_actans)**2\n",
    "        Cd_rmse = np.sqrt(np.sum(Cd_mse)/len(Cd_mse))\n",
    "        Cd_nrmse = (Cd_rmse/np.mean(Cd_actans))*100\n",
    "        mass_mse = (mass_out - cur_df['mass'][test_ids])**2\n",
    "        mass_rmse = np.sqrt(np.sum(mass_mse)/len(mass_mse))\n",
    "        Vol_out_s = 4*np.pi/3*(sph_outs/(4*sar_outs) * cur_df['area'][test_ids]/np.pi)**(3/2)\n",
    "        rho_s = mass_out/Vol_out_s\n",
    "        Vol_out_c = 4*np.pi/3*(c_sph_outs*Acrat_outs * cur_df['area'][test_ids]/np.pi)**(3/2)\n",
    "        rho_c = mass_out/Vol_out_c\n",
    "        Vol_out_l = 4*np.pi/3*(l_sph_outs*(1/(2*sar_outs)-Alrat_outs) * cur_df['area'][test_ids]/np.pi)**(3/2)\n",
    "        rho_l = mass_out/Vol_out_l\n",
    "        predout = pd.DataFrame(por_outs,columns=['por'])\n",
    "        predout['sar'] = sar_outs\n",
    "        predout['sph'] = sph_outs\n",
    "        predout['lsph'] = l_sph_outs\n",
    "        predout['csph'] = c_sph_outs\n",
    "        predout['Acrat'] = Acrat_outs\n",
    "        predout['Alrat'] = Alrat_outs\n",
    "        predout['Df_out'] = Df_outs\n",
    "        predout['Cdout'] = Cd_outs\n",
    "        predout['massout'] = np.array(mass_out)\n",
    "        predout['Rhos'] = np.array(rho_s)\n",
    "        predout['Rhoc'] = np.array(rho_c)\n",
    "        predout['Rhol'] = np.array(rho_l)\n",
    "        predout['id'] = np.array(cur_df['id'][test_ids])\n",
    "        predout['Cd'] = np.array(cur_df['Cd'][test_ids])\n",
    "        predout['Re'] = np.array(cur_df['Re'][test_ids])\n",
    "        predout['Df'] = Df_ans\n",
    "        predout['mass'] = np.array(cur_df['mass'][test_ids])\n",
    "        hs_planout = pd.concat([hs_planout,predout],ignore_index=True)\n",
    "\n",
    "if correlation == 3:\n",
    "    hs_planout = pd.DataFrame(columns=['Cdout','massout','id','Cd','Re','mass'])\n",
    "    x,y,z,w,v = kf.split(fulldata)\n",
    "    model = KGCNN3.to(device)\n",
    "    for i in range(5):\n",
    "        model.load_state_dict(torch.load(dir_model+str(i)+\".pth\"))\n",
    "        if i == 0:\n",
    "            test_loader = DataLoader(\n",
    "                dataset=fulldata,\n",
    "                batch_size=1,\n",
    "                sampler=torch.utils.data.SubsetRandomSampler(x[1])\n",
    "            )\n",
    "        elif i == 1:\n",
    "            test_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=1,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(y[1])\n",
    "            )\n",
    "        elif i == 2:\n",
    "            test_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=1,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(z[1])\n",
    "            )\n",
    "        elif i == 3:\n",
    "            test_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=1,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(w[1])\n",
    "            )\n",
    "        elif i == 4:\n",
    "            test_loader = DataLoader(\n",
    "            dataset=fulldata,\n",
    "            batch_size=1,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(v[1])\n",
    "            )\n",
    "        model.eval()  # Set the model in evaluation mode\n",
    "\n",
    "        Cd_outs = []\n",
    "\n",
    "        Cd_ans = []\n",
    "\n",
    "        test_ids = []\n",
    "        with torch.no_grad():\n",
    "            for id,images,A_char,Re,Cd in test_loader:\n",
    "\n",
    "                id = id.to(device)\n",
    "                images = Variable(images, requires_grad=True)\n",
    "                A_char = Variable(A_char, requires_grad=True)\n",
    "                Re = Variable(Re, requires_grad=True)\n",
    "                Cd = Variable(Cd, requires_grad=True)\n",
    "\n",
    "                A_char = A_char.view(A_char.size(0),1)\n",
    "                Re = Re.view(Re.size(0),1)\n",
    "                Cd = Cd.view(Cd.size(0),1)\n",
    "\n",
    "                images = images.to(device)\n",
    "                A_char = A_char.to(device)\n",
    "                Re = Re.to(device)\n",
    "                Cd = Cd.to(device)\n",
    "\n",
    "                #with autograd.detect_anomaly():\n",
    "                # Forward pass\n",
    "                cd_out = model(images,Re)\n",
    "\n",
    "                Cd_outs.append(torch.exp(cd_out).item())\n",
    "\n",
    "                Cd_ans.append(torch.exp(Cd).item())\n",
    "\n",
    "                test_ids.append(id.item())\n",
    "                #err += (abs(outputs - t_Cd)/t_Cd)*100\n",
    "                #print(err)\n",
    "                #loss =  l + loss_mean\n",
    "        #outs_act = np.array(outs)\n",
    "        #answers_act = np.array(answers)\n",
    "        Cd_outs = np.array(Cd_outs)\n",
    "        Cd_acts = np.array(Cd_outs)\n",
    "\n",
    "        Cd_ans = np.array(Cd_ans)\n",
    "        Cd_actans = np.array(Cd_ans)\n",
    "\n",
    "        test_ids = np.array(test_ids)\n",
    "        mass_out = Cd_acts*0.5*cur_df['rho'][test_ids]*(cur_df['vel'][test_ids]**2)*cur_df['area'][test_ids]/9.81\n",
    "        Cd_errs = ((Cd_ans - Cd_outs)/abs(Cd_ans))*100\n",
    "        Cd_errs_abs = (abs(Cd_ans - Cd_outs)/abs(Cd_ans))*100\n",
    "        Cd_aerrs = -((Cd_actans - Cd_acts)/abs(Cd_actans))*100\n",
    "        Cd_aerrs_abs = (abs(Cd_actans - Cd_acts)/abs(Cd_actans))*100\n",
    "        Cd_mse = (Cd_acts - Cd_actans)**2\n",
    "        Cd_rmse = np.sqrt(np.sum(Cd_mse)/len(Cd_mse))\n",
    "        Cd_nrmse = (Cd_rmse/np.mean(Cd_actans))*100\n",
    "        mass_mse = (mass_out - cur_df['mass'][test_ids])**2\n",
    "        mass_rmse = np.sqrt(np.sum(mass_mse)/len(mass_mse))\n",
    "        predout = pd.DataFrame(Cd_outs,columns=['Cdout'])\n",
    "        predout['massout'] = np.array(mass_out)\n",
    "        predout['id'] = np.array(cur_df['id'][test_ids])\n",
    "        predout['Cd'] = np.array(cur_df['Cd'][test_ids])\n",
    "        predout['Re'] = np.array(cur_df['Re'][test_ids])\n",
    "        predout['mass'] = np.array(cur_df['mass'][test_ids])\n",
    "        hs_planout = pd.concat([hs_planout,predout],ignore_index=True)\n",
    "# save outputs to csv file\n",
    "hs_planout.to_csv(dir_savdat)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
